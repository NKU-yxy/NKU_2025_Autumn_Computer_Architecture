# 体系结构知识点总结

<center><b>Author: YXY && ZKY && ChatGPT</b> </center>



[TOC]

## Chapter 0 Final 题型分布

题目&分数：

- 单选题 30‘
- 简答题 20’
- 大题（4题）50‘

---

## Chapter 1 流水线

### CPU性能指标

#### CPI 单指令周期数

定义： 平均下执行一条指令所需要的周期数

`IPC = 1 / CPI` （后面一般都考虑IPC而不是CPI）

不同指令需要的周期数不同,eg. add需要一个周期，而divide需要10多个周期；

某程序的CPI取决于该程序不同指令的占比。

---

#### CPU Time 程序执行时间

- **程序执行时间 = 指令数（IC） * 单指令执行周期数 （CPI）* 单周期时间（时钟周期=1/时钟频率）**
- 程序执行的指令数（Instruction Count）
  - 由程序本身、编译器、指令集（ISA）决定
- 单指令执行周期数（CPI, Cycles Per Instruction）
  - 执行一条指令平均需要多少周期
  - 由程序本身、编译器、指令集（ISA）、体系结构决定
- 单周期时间（Clock Period）
  - 倒数是时钟频率：0.5 Ghz to 4 Ghz
  - 由体系结构、制造工艺等因素决定

---
### 单周期 CPU总结

- **每条指令只需要一个周期完成**
  - CPI = 1
- **每条指令的执行时间取决于最慢指令**的执行时间
  - 尽管很多指令不需要那么长时间
- 系统**时钟周期取决于完成最慢指令**所需的时间
  - 设计的关键路径由最慢指令的处理时间决定

------

### 简单五级流水线

#### 流水线基本概念

核心思想：**提升指令吞吐量，而不是去降低单个指令的执行时间**

**指令吞吐量？**

单位时间内，CPU完成的指令数量（即每秒能完成多少条指令）

**流水线做法？**

- 当指令从第1阶段进入到第2阶段, 下一条指令进入第1阶段；
- 每条指令顺序经过所有阶段；
- 所有指令经过的阶段数相同；
- 并行方式: “指令-阶段 并行”

**流水线优点？**

相比单周期CPU，指令进入/离开的频率更快

---

#### 为什么流水线时钟周期 > (单周期的时钟周期) / (流水段个数) ？

- 三个原因：
  - 每个流水段都增加了寄存器 → 增加延迟
  - 每段时间长度不同 → 时钟周期按最长段计算
  - 增加了其他线路（前递/旁路等）
- 备注：**流水线的级数增加时，频率增益会减少（overhead 更高，即会引入额外开销）**

---

#### 流水线为什么 CPI > 1 ? 

- 五级流水线时钟周期：
  - `Clock = MAX(Tinsn-mem, Tregfile, TALU, Tdata-mem, Twriteback)`

- 对于简单流水线（scalar in-order pipeline）：
  - `CPI = 1 + stall 惩罚`
- 而`Stall` 用来解决流水线中的危害（hazards）：
  - Hazard：影响流水线正确执行的问题（数据相关或控制相关）
  - Stall：为了正确执行而引入的停顿

---

分析示例：

- 单周期：时钟周期 = 50ns，CPI = 1，Performance = 50ns/insn

- 5 级流水线：
  - 时钟周期 = 12ns（原因：50ns/5 + 额外开销）
  - 若 CPI = 1：Performance = 12ns/insn
  - 实际 CPI 通常 > 1（stall）：例 CPI = 1.5 → Performance = 18ns/insn
  - 仍然比单周期性能高很多。
  

---

### 指令相关性以及危害

#### 指令相关性：

##### 数据相关性（key）

两条指令使用相同的存储位置（eg.相同寄存器）

三种类型： 

- **写后读（RAW，真相关）**
- 写后写（WAW）
- 读后写（WAR）

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102180430190.png" alt="image-20260102180430190" style="zoom: 67%;" />



RAW是真正需要解决的数据相关性，而其他两种相关性都是因为寄存器不够多产生的。



**那该如何解决呢？**

- 暂停流水线，一直到前一条将结果计算出来为止（通过插入Stall来解决）
- 数据计算完成后，立马将数据前递到先前的流水线阶段（Bypass/旁路/前递/forward四者都指这种）

---

##### 控制相关性

一条指令会影响另一条指令是否能够执行 eg. 分支跳转/中断

---

##### 结构相关性

两条指令在同时去使用同一个资源，造成冲突和抢占

---

#### 危害（Hazard）

**有些相关性会导致指令执行错误**

eg：一条指令以前一条指令的结果作为操作数，但前一条指令的结果尚未写入寄存器或内存，导致结果错误

---

## Chapter2 存储层次

内存目标：**既快又大的内存**

**如何实现？**

采用**多层次**的存储结构：

- 离CPU越远，空间越大，速度越慢；
- 然后保证CPU需要的大部分数据都尽量保存在速度快的层中（离CPU近的地方）

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102193932607.png" alt="image-20260102193932607" style="zoom:50%;" />

---

### 常见存储（重点ppt没但感觉挺重要）

DRAM vs SRAM

**DRAM 断电会丢失数据，SRAM 断电也会丢失数据**。它们都属于**易失性存储（volatile）**。

DRAM

- **断电会丢数据：是**
- 是否需要刷新：需要（通电时也要周期性 refresh）
- 存储单元：电容 + 晶体管（用电容存电荷表示 0/1）
- **访问速度：相对慢**
- **密度/容量：高（更省面积，单位面积能做更多 bit）**
- **价格：低**
- 常见用途：主存（内存条）

SRAM

- **断电会丢数据：是**
- 是否需要刷新：不需要（只要不断电就能稳定保持）
- 存储单元：触发器结构（通常 6T，约 6 个晶体管存 1 bit）
- **访问速度：快**
- **密度/容量：低（更占面积）**
- **价格：高**
- 常见用途：CPU Cache（L1/L2/L3）

一句话

- DRAM：便宜大容量，但要刷新、慢一些
- SRAM：贵小容量，但快、无需刷新（但同样断电就没了）



易失性存储 VS 非易失性存储

断电丢数据的存储，就是**易失性存储**；

断电不会丢失数据的存储，统称 **非易失性存储（NVM）**。特点：**不电也能把数据长期保存**。

- DRAM/SRAM：断电就没（易失）
- SSD/U盘/HDD/ROM：断电还在（非易失）

---

### 内存局部性（时间&空间）

内存局部性：

- 一个典型程序的内存访问会表现出很多局部性（一般程序都有很多“循环”）
- 时间局部性：一个程序倾向于在一个小的时间窗口内多次访问相同的内存位置
- 空间局部性：一个程序倾向于一次访问一片相近的内存位置
  - 指令内存的访问（指令的地址通常是连续的）
  - 数组访问

**大白话解释**

- **局部性**就是：程序访问内存不是“到处乱跳”，而是**反复用同一块**，或者**顺着用附近的一片**。这也是缓存（cache）能有效的根本原因。
- **时间局部性**：刚用过的东西，过一会儿大概率还要用（例如循环里反复读写同一个变量）。
- **空间局部性**：你访问了某个地址，接下来大概率会访问它旁边的地址（例如顺序执行指令、按下标从小到大扫数组）。

------

PPT例子：

```c
int sum = 0;
int X[1000];
for(int c = 0; c < 1000; c++){
  sum += X[c];
}
```

- 哪里表现出空间局部性?
- 哪里表现出时间局部性?

**大白话解释**

- **空间局部性最明显：`X[c]`**
  - 访问顺序是 `X[0], X[1], X[2]...`，内存地址也是一段连续的，所以“访问了一个元素，很可能马上访问它旁边的元素”。
- **时间局部性最明显：`sum` 和 `c`**
  - `sum` 每次循环都要读/写一次；`c` 每次循环都要比较、加 1、再比较。它们在很短时间里被反复用到。
- **指令本身也有空间局部性**：这段循环的机器指令一般在内存里连续排放，CPU取指也会“顺着取”。

---

**利用时间局部性**

- 思路：将最近访问的数据存放在访问速度快的地方（cache）
- 期望：数据很快会被再次访问

**大白话解释**

- 既然“刚用过的很可能马上还要用”，那就把**刚用过的数据**先放进更快的地方（cache）。
- 这样下一次再用同一个数据，就不用去慢的内存拿了，直接从cache秒取。

------

**利用空间局部性**

- 思路：将与近期访问的数据地址相邻的数据放在访问速度快的地方
  - 逻辑上将内存划分成大小相等的块（blocks）
  - 访问某个数据，就将数据所在的块整个缓存
- 期望：相邻的数据很快会被访问

**大白话解释**

- 既然“用到一个地址，旁边的也很可能马上要用”，那就别只拿一个字节/一个int：**直接把这一片一起搬进cache**。
- 这“一片”就叫 **block / cache line（缓存行）**：内存被“按固定大小切块”，每次缓存是**整块进出**。
- 结果：你读了 `X[0]`，cache 可能顺带把包含 `X[0]、X[1]、X[2]...` 的那一整块都带上；接下来访问 `X[1]` 很可能直接命中cache。

---

### Cache(Key)

#### **Cache 基本概念**

- 基本概念
  - Cache 里最小搬运单位叫 **Block / Cache Line**：Cache 和主存都“逻辑上”按 block 切块，数据以 block 为单位进出 cache。
  - 访问时两种结果：**Hit**（在 cache 里，直接用）/ **Miss**（不在 cache 里，要从内存拷进来）。
  - 设计 cache 主要要做 3 个决定：**放哪里（Placement）**、**替换谁（Replacement）**、**写策略（Write policy）**。

------

**Cache 寻址（怎么用地址找到 cache 里那一格）**

- 提取要点
  - 内存按固定大小切成 blocks；每个 block 映射到 cache 的某个位置，由地址里的 **index bits** 决定。
  - 访问流程：
    1. 用 **index bits** 先定位到某个 cache block（里面有 tag 和 data）
    2. 看 **valid bit**（这一格是否装着“有效数据”）
    3. 比较地址里的 **tag bits** 和 cache 里存的 tag 是否一致
  - 命中条件：valid bit=1 且 tag 相同。
- 大白话解释
  - 一个内存地址会被拆成三段：
    - **index**：告诉你“去 cache 的第几号格子/第几组”
    - **tag**：告诉你“这一格子里装的是不是你要的那一箱”
    - **offset（byte in block）**：告诉你“箱子里第几个字节/第几个位置”
  - 先靠 index 找位置，再靠 tag “对暗号”，最后用 offset 从那一箱里取具体数据。

------

**放置策略——直接映射（Direct-Mapped）**

- 提取要点
  - 例子设定：主存 256B、block 8B ⇒ 主存 32 个 blocks；cache 64B、也按 8B 一块 ⇒ cache 8 个 blocks。
  - **直接映射**：每个主存 block **只能** 放到 cache 里 **唯一** 的一个位置。
  - 如果两个地址的 index 一样，它们就会抢同一个 cache 位置 ⇒ **冲突未命中（conflict miss）**。
- 大白话解释
  - 直接映射就像“每个快递只能放到固定编号的柜子”：
    - 你这个包裹编号算出来必须放 3 号柜，不能放别的柜。
  - 坏处：如果两个不同包裹“都被算到 3 号柜”，就会互相顶掉——你一会儿拿 A，一会儿拿 B，柜子里永远只能留一个，命中率可能非常差。
  - 好处：硬件简单、访问快（因为不用在多个位置里找）。

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102201439518.png" alt="image-20260102201439518" style="zoom: 67%;" />

------

**直接映射的典型灾难（A、B 交替访问 ⇒ 0% 命中）**

- 提取要点
  - index 相同但 tag 不同的两个 blocks（地址 A、B）不能同时在 cache 中出现。
  - 如果访问序列是 A, B, A, B, A, B… 会一直冲突 ⇒ **全部 miss**。
- 大白话解释
  - 你把抽屉的某一格当成“单人宿舍”，A 来住就把 B 赶走，B 来住又把 A 赶走。
  - 交替访问就是“不断换人入住”，每次都要从仓库搬一次，命中率直接变 0。

------

**放置策略——组相联（Set Associativity）**

- 提取要点
  - 直接映射里某些地址（例：0 和 8）总冲突。
  - 组相联：不是一列 8 个块，而是分成多列（例：2 列，每列 4 个块）——本质是“每个 index 对应一个 set，set 里有多个位置”。
  - 关键思想：**index 先映射到 set，set 内可以任选一个位置放** ⇒ 冲突未命中减少。
  - 代价：更复杂、更慢、tag 更长（因为 index bits 变短了）。
- 大白话解释
  - 直接映射：一个 index 只能住“单人间”。
  - 组相联：一个 index 对应一个“多人间（set）”，里面有好几张床（ways）。
  - A 和 B 即使 index 一样，也可能分别睡不同床位，就不会互相赶走。
  - 代价是你要在房间里“多看几张床的门牌(tag)”，硬件更复杂，时间也可能更长。

下图关联度 = 2：可以看出，从原先从上到下8行（8个cache line/block）改为了 2组——1组4个cache line/block,这样就会让index 从原先 mod 8 变为 mod 4,从3bits变为2bits；

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102203047048.png" alt="image-20260102203047048" style="zoom:67%;" />

------

**更高关联度（例如 4-way）**

- 提取要点
  - 关联度越高（例如 4-way），冲突未命中更少。
  - 代价：tag 比较器更多、data 选择器（MUX）更宽、tag 更长。
- 大白话解释
  - “多人间床位更多”当然更不容易打架（更少冲突 miss）。
  - 但你要同时检查更多床位的“门牌号(tag)”才能确认命中，硬件堆上去，成本和延迟都会涨。

下图 关联度 = 4：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102203423016.png" alt="image-20260102203423016" style="zoom:67%;" />

------

**放置策略——全相联（Full Associativity）**

- 提取要点
  - 全相联：一个内存 block 可以放在 cache 的 **任何位置**。
- 大白话解释
  - 相当于“整个 cache 是一个大通铺”，新来的箱子想放哪儿都行——基本把“固定位置导致的冲突”问题压到最低。
  - 但代价最大：每次查找时理论上要“到处比 tag”，实现最复杂、可能也最慢（所以通常只在很小的结构里用，比如某些 TLB/小缓存）。

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102203449312.png" alt="image-20260102203449312" style="zoom:67%;" />

------

**把 18–24 页串成一句人话总结**
Cache 就是 CPU 的“小抽屉”，数据按“箱子（block）”搬；访问时先用 index 找到候选位置/房间，再用 tag 对暗号决定 hit/miss。放置策略从“一个 index 只能放一个（直接映射）”到“一个 index 可以放多个（组相联/更高路数）”，再到“随便放（全相联）”：**冲突越少，硬件越复杂、可能越慢**。

---

#### 例题讲解Index&Tag&Offset

给一个例题数据：

例题： 假设内存是字节寻址且其的大小为256 bytes, block大小为 8 bytes， 假设Cache大小为64 bytes，采取直接映射

解：

先把“标准做法”写成一套固定公式。

已知

- 内存字节寻址，大小 256B ⇒ 地址是 0~255（8 位地址）
- block 大小 B = 8B
- cache 大小 C = 64B ⇒ cache line 数 L = C/B = 64/8 = 8 行（direct-mapped 时就是 8 个槽位）

通用中间量

- **block_number = ⌊ address / B ⌋**（地址属于第几个内存块）
- **offset = address mod B**（块内偏移）

直接映射（Direct-Mapped）的正确计算

- **index = block_number mod L**
- **tag = ⌊ block_number / L ⌋**

把它展开成只用 address 的形式

- index = ⌊address / B⌋ mod L
- tag = ⌊address / B⌋ / L
- offset = address mod B

---

用一个具体地址走一遍

取 address = 58（0x3A）

- block_number = 58 / 8 = 7
- offset = 58 mod 8 = 2
- index = 7 mod 8 = 7
- tag = 7 / 8 = 0

取 address = 186（0xBA）

- block_number = 186 / 8 = 23
- offset = 186 mod 8 = 2
- index = 23 mod 8 = 7
- tag = 23 / 8 = 2
  你会看到：两个地址 offset 相同、index 相同、tag 不同 ⇒ 直接映射下会抢同一行。

------

**如果改为全相联（Fully Associative）？**

**offset 不变，index 基本消失，tag 变大**。

原因：全相联 = 任意内存块可以放到 cache 的任意一行，所以不需要用 index 去“固定定位某一行/某一组”。

全相联（Full Associative）

- L 还是 8 行，但不再用 index 选行
- **offset = address mod B**（不变）
- **tag = block_number**（或等价写作 ⌊address / B⌋）
- **index：没有（或认为 index bits = 0）**

直观理解：

- 直接映射：先用 index 指到唯一那一行，再用 tag 验证
- 全相联：没有唯一行，要“在所有行里比 tag 找匹配”（硬件用并行比较/内容寻址的方式做）

---

**如果是 k-way 组相联（Set-Associative）？**

- num_sets = L / k
- offset = address mod B
- index = block_number mod num_sets
- tag = block_number / num_sets

---

#### Cache主要参数影响

##### Capacity

- Cache size：总容量
  - 更大的缓存可以更好地利用时间局部性

- Cache增大会增加访问延迟
  - 越小越快 ⇒ 越大越慢
  - 访问延迟可能会增加关键路径的延迟

- Cache太小
  - 无法有效利用时间局部性
  - 有用的数据频繁被替换（hit rate 下降）

**概念解释**

- **Cache size / Capacity（容量）**：cache 能同时保存多少字节的数据。容量越大，能同时“保留”的不同内存块越多。
- **时间局部性**：程序在较短时间内重复访问同一地址/同一数据结构的概率高。容量大时，这些“刚用过的数据块”更可能仍在 cache 中，从而减少 miss。
- **访问延迟（hit latency）**：命中时从 cache 取到数据需要的时间。cache 变大后，常见原因会让 hit latency 上升：
  - tag 数量更多、比较/选择路径更长
  - 结构更复杂（更大的阵列、更长的线、更复杂的译码/多路选择）

变化图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102205141151.png" alt="image-20260102205141151" style="zoom: 80%;" />

---

##### 关联度

- 关联度：每个组(set)里能放几个blocks
- 关联度更高
  - 命中率更高
  - 访问时间更长
  - 硬件更复杂（更多比较器）
- 随着关联度增加，命中率增长趋缓

**概念解释**

- **组相联（set-associative cache）**：cache 被分成很多个 set，每个 set 里有多个位置（way）。
- **为什么关联度更高命中率更高**：
  - 直接映射时，一个内存块只能放到唯一一行，容易发生不同块映射到同一行而互相顶掉 → conflict miss（冲突未命中）。
  - 关联度提高后，同一个 set 里有多个 way，多个“映射到同一 set 的块”可以同时存在，从而减少冲突未命中。
- **为什么访问时间更长、硬件更复杂**：
  - 命中判断需要把请求地址的 tag 与该 set 内所有 way 的 tag 做比较（并行比较器更多）。
  - 命中后还要在多个 way 的数据输出中选择正确的那个（多路选择器更大）。
- **为什么命中率提升会趋缓**：
  - 从 1-way 到 2-way 往往能显著减少冲突；再往上增加 way，冲突继续减少，但占比越来越小，收益边际下降。

图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102205323871.png" alt="image-20260102205323871" style="zoom: 80%;" />

---

Block Size

- 增大block size有正反两方面影响
  - 正面：空间预取（地址相邻的数据被提前带上来），有利于降低miss rate
  - 反面：产生干扰
    - 能放入cache的blocks变少了
    - 可能增加miss rate
    - 特殊情况：全局只有1个block
- 两方面影响同时存在，哪个更大取决于workload

**概念解释**

- **block size 变大带来的好处（利用空间局部性）**：
  - **空间局部性**：访问某地址后，随后访问相邻地址的概率高（顺序取指、顺序访问数组等）。
  - line 大时，一次 miss 会把更大范围的相邻数据一起带入 cache，后续访问相邻地址更容易 hit → miss rate 下降。
- **block size 变大带来的坏处（干扰/容量效果变差）**：
  - cache 总容量固定，line 变大 ⇒ cache 中能容纳的 line 数变少。
  - line 数变少会更容易把仍会用到的 line 换出，导致更多 capacity/conflict 类的 miss。
  - 还可能带来 **无用数据搬入**：程序只用到 line 中很小一部分，但仍把整条 line 搬入，占用容量与带宽。
- **workload 决定哪边更占主导**：
  - 若访问模式强空间局部性（连续扫描），更大 line 往往更好；
  - 若访问离散、随机、或工作集很大，过大 line 可能反而更差。

图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102205414618.png" alt="image-20260102205414618" style="zoom:80%;" />

---

#### 替换策略

- Cache miss发生时，set中的哪个block将被替换?
  - 优先替换 invalid block（还没放置任何数据）
  - 如果都是 valid，由替换策略决定：
    - Random
    - FIFO（first-in first-out）
    - LRU（least recently used）
    - NMRU（not most recently used）
    - 2路组相联时就是 LRU
    - Belady’s（最优但无法实现）

**概念解释**

- 常见策略含义：
  - **Random**：随机选一个 way 淘汰，实现简单，但命中率不一定好。
  - **FIFO**：淘汰最早进入该 set 的 line；不考虑是否最近用过。**（考试可能考）**
  - **LRU**：淘汰“最近最久未被访问”的 line；利用时间局部性假设“最近用过的更可能再用”。（**考试大概率考**）
  - **NMRU**：保证“不淘汰最近刚用过的那条”，其它从剩余里选；是 LRU 的近似，硬件成本更低。
  - **2-way 时 LRU 很容易实现**：只需要 1 个 bit 记录“最近用的是哪一路”。
  - **Belady’s optimal**：淘汰未来最晚才会被访问的 line；理论最优但需要知道未来访问序列，所以实际不可实现，只用于分析下界。

---

#### 写操作如何传递（Cache Hit）

两种处理方式：

- 写穿透(Write-through)：立即传递
- 写返回(Write-back)：当 block 被替换时再写
  - 需要 dirty 位
  - 替换 clean block：无需额外操作
  - 替换 dirty block：需要向下一层写

**概念解释**

- 这里讨论的是 **写命中（write hit）**：要写的地址已经在 cache 里。关键问题是：**更下层（下一层 cache 或内存）什么时候也更新？**
- **Write-through（写穿透）**：
  - 写命中时：更新本层 cache，同时立刻把同样的写操作发送到下一层。
  - 结果：下层数据始终保持最新（更容易保持一致性），但写流量大。
- **Write-back（写返回）**：
  - 写命中时：只更新本层 cache，并把该 line 标记为 **dirty=1**（表示本层比下层更新）。
  - 只有当该 line 被替换出 cache（evict）时，才把整条 line 写回下一层。
  - **clean line（dirty=0）**：未被修改，替换时不用写回。
  - **dirty line（dirty=1）**：被修改过，替换时必须写回，否则会丢失最新数据。

**两种策略对比：**

- 写穿透：需要额外带宽；下层要处理小写请求；不需要 dirty bits；GPU 常用
- 写返回：带宽使用少；CPU 常用

**概念解释**

- 写穿透为什么“带宽压力大”：
  - 程序如果频繁对同一 cache line 做很多次小写（1/2/4/8B），每次都要向下层发请求 → 下层总写流量显著增加。
  - 下层还必须支持“部分写（partial write）”或读-改-写等机制处理小粒度更新。
- 写返回为什么“带宽更省”：
  - 多次对同一 line 的写在上层被合并，最后只在换出时写回整条 line（通常几十字节） → 总下行写流量更小。

------

#### 处理 Write Miss

- 写分配(Write-allocate)：先把 block 从下一层读上来，再写
- 写不分配(Write-non-allocate)：直接往下一层写

**概念解释**

- 这里讨论的是 **写未命中（write miss）**：要写的地址不在 cache 中。此时决定“要不要把对应的 cache line 先装进来”。
- **Write-allocate（写分配 / fetch-on-write）**：
  - 先把该地址所在的整条 line 从下层读入 cache（会产生一次读流量），再在 cache 里完成写。
  - 优点：如果后续还会读/写同一 line，能提高命中率。
  - 缺点：即使只是“写一次就不再用”，也会多一次把 line 读上来的开销。
  - 常与 **write-back** 搭配：写入后先在上层积累，换出时再整条写回。
- **Write-non-allocate（写不分配 / write-around）**：
  - 不把 line 读入 cache，直接把写操作发到下一层。
  - 适合“写后很少再读”的模式，可避免无意义的读入占带宽与占 cache 容量。

---

### 提升cache性能

- 程序执行时间 = 指令数 * CPI * 时钟周期
- CPI (with cache) = CPI_base + CPI_cachepenalty
- CPI_cachepenalty = …
  1. 降低 miss rate
  2. 降低 miss penalty
  3. 降低 hit latency

#### 降低miss rate

- Cache Miss 的三种情况 (3C)
  - Compulsory miss
    - 第一次访问一个 block 肯定会发生 miss
    - 后续对该 block 的访问应该命中，除非 block 因为下面的原因被替换
  - Capacity miss
    - cache 空间不足以缓存所有的数据（小于内存）
    - 即使采用大小相等的全相联 cache、以及最优的替换算法，仍然会发生的那些 miss
  - Conflict miss
    - 既不是 compulsory miss，也不是 capacity miss 的那些 miss

##### 增大Cache size

- 可以降低 conflict miss 和 capacity miss
- 缺点：访问延迟增加；能耗更高

##### 增加关联度

- 可以降低 conflict miss
- 缺点：访问延迟增加

##### 增加 block size

- 可以降低 compulsory miss
- 缺点：如果访存空间局部性不高，会浪费 cache 空间

------

详细解释

**为什么把 CPI 拆成 “CPI_base + CPI_cachepenalty”**

- CPI_base：假设访存都很顺利（或把存储系统影响抽掉）时，流水线本身/指令组合导致的基础 CPI，例如结构冲突、分支代价（如果已计入 base）等。
- CPI_cachepenalty：由于 cache 行为导致的额外周期，主要来自 miss 引起的停顿（处理器需要等待数据/指令从更低层取回）。
  拆开后便于针对性优化：你可以保持流水线不变，只优化 cache（降低 penalty），或者反过来。

**三个可优化方向分别在改什么**

- 降低 miss rate（未命中率）：让访问更可能命中 cache。miss rate = misses / accesses。miss rate 降低，会减少“需要去下层取数据”的次数。
- 降低 miss penalty（未命中代价）：一次 miss 发生后，从下层把数据拿回来、并让处理器能继续执行所需的额外时间（周期数）。同样 miss 次数下，penalty 越小越好。
- 降低 hit latency（命中延迟）：命中时读/写 cache 需要的时间（周期）。命中是最常见路径，hit latency 变大可能直接拉长时钟周期或增加 load-use 等等待。

**3C miss 的严格含义与区分方式**

- Compulsory miss（强制/冷启动 miss）：某个 block 第一次被访问时一定 miss，因为 cache 里还没有它。与 cache 大小/相联度/替换策略无关（首次必 miss）。
- Capacity miss（容量 miss）：工作集（程序在一段时间内反复用到的数据集合）大于 cache 能容纳的总量，即使你把 cache 做成同容量的全相联并使用最优替换，仍会 miss。这类 miss 的根因是“总容量不够”。
- Conflict miss（冲突 miss）：容量理论上够，但由于放置限制（例如直接映射/组相联 set 数有限），一些本可共存的 block 被迫映射到同一 set，导致互相挤掉而 miss。这类 miss 的根因是“映射冲突 + 相联度有限”。

**为什么“增大 cache size / 增加关联度 / 增加 block size”会对应影响不同类型 miss**

- **增大 cache size：**
  - 能装下更多不同 block，所以工作集更可能放得下 → capacity miss 下降。
  - set 数通常也会增加（在 block size 和相联度不变时），映射冲突相对缓解 → conflict miss 也可能下降。
  - 代价：更大的 tag/data 阵列、更多连线/更长关键路径，可能导致 hit latency 增加；并且读写更大结构的能耗更高。
- **增加关联度（同一 set 允许放更多 ways）：**
  - 同一 set 内能共存的 block 更多，很多原本因为“只能放 1 个或太少”而发生的冲突被消除 → conflict miss 下降。
  - 代价：需要更多 tag 比较器（并行比较多个 way 的 tag）、更复杂的选择数据通路（MUX 更宽）、替换状态更复杂，通常会增加 hit latency。
- **增加 block size（一次 miss 填充更大块）：**
  - 如果程序后续会访问相邻地址（空间局部性），把相邻数据一起带上来可以减少“首次访问相邻数据”时的 compulsory miss。
  - 风险点（文中用 “Conflict misses ?” 表示不一定单调）：block 变大意味着在同样 cache 总容量下能容纳的 block 数变少，可能导致更多 capacity/conflict 类 miss（能放的“不同块”变少、set 压力变大）。
  - 代价：若空间局部性不足，被带上来的很多字节不会被用到，占用容量并消耗带宽，造成浪费。

图（Lys说不考，大概看看就行）：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103103455665.png" alt="image-20260103103455665" style="zoom:67%;" />

---

## Cache 期末题目（大题其一，Hw2）

Hw2:

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102203829827.png" alt="image-20260102203829827" style="zoom: 80%;" />

---

解：

**解题思路？**

- 一般就是按照题目给你的顺序，从块大小从上到下去确定各个选项的选择，eg. 这一题就是先从块大小开始选择；
- 理解清楚概念，然后仔细计算就好，不是那么难的，如果不确定的就直接写无法确定就ok；
- 看清题目：eg. 是不是连续访问/还是访问一个清空cache？一开始cache为不为空？仔细看题。

---

**思考过程？**

**先确定块大小：**

**这种一般看完第一个序列就知道了**， eg. 我们看到第一个序列，访问的地址是 0 16 24 25 1024 255 1100 305 且 cache hit 为 2/8 命中了两次，我们不难想象，无论块大小(block size)为 8/16/32/64/128 Bytes，都能保证，25是一定cache hit 的。

为什么呢？

因为在访问25前，刚访问24,24访问后是一定在cache内的，所以25紧跟着24，一定会cache hit！

然后再想想，看看有没有不可能的大小？

如果假设block size 为 32/64/128 Bytes?

是不是很容易想到，当访问0后，访问16/24/25的时候都应该cache hit。但访问8个地址一共就hit两次，所以直接排除32/64/128 Bytes了。

那究竟是8 or 16 Bytes呢？

再想想，如果是16Bytes大小的block的话，会发生以下问题：当访问0时，对应`block id= 0/16 =0`,然后访问16的时候，`block number = 16 /16 = 1`,然后访问24和25的时候也都对应block 1，所以完美满足hit 2/8；

那如果是8Bytes大小呢？

我们可以看到 cache可以选的是4KB/8KB，至少也有多少个block呢？ 

`block 数量 = cache size / block size = 4KB / 8B = 512 个`

可以知道，block数量至少有512个，然后如果选了block size=8B,就没办法实现hit 2次了，只能hit 1次。因为0对应block 0, 16对应block 2, 24和25对应block 3,  1024对应block 128(128<512), 255对应block 31， 1100对应block 137, 305对应block 38,只能hit一次。

**所以答案： Block size = 16 Bytes**

---

第二题详细数据：

- 块大小 B = 16 B
- Cache 总大小 C ∈ {4KB, 8KB}
- 相联度 A ∈ {2,4,8}
- Cache 行数（总块数）L = C / B
- set 数 S = L / A

对任意字节地址 addr：

- block_number = ⌊addr / B⌋
- offset = addr mod B （块内偏移，0~15）
- set_index = block_number mod S
- tag = ⌊block_number / S⌋

------

把序列 1、2 先换成 block_number（这一步与你选不选相联度无关）

序列1地址：0, 16, 24, 25, 1024, 255, 1100, 305
除以 16 取整得到 block_number：

- 0 -> 0
- 16 -> 1
- 24 -> 1
- 25 -> 1
- 1024 -> 64
- 255 -> 15
- 1100 -> 68
- 305 -> 19

所以序列1访问块序列：0, 1, 1, 1, 64, 15, 68, 19
命中只来自 “同一块重复访问”：24、25 这两次命中 ⇒ 2/8 正好对上。

------

关键：用序列2的命中率 3/8 去反推相联度

序列2地址：31, 65536, 65537, 131072, 262144, 8, 305, 1060
对应 block_number：

- 31 -> 1
- 65536 -> 4096
- 65537 -> 4096
- 131072 -> 8192
- 262144 -> 16384
- 8 -> 0
- 305 -> 19
- 1060 -> 66

所以序列2访问块序列：1, 4096, 4096, 8192, 16384, 0, 19, 66

如果“没有发生替换冲突”，那么基于序列1结束时 cache 里已有 {0,1,15,19,64,68}，序列2本来应该有这些命中：

- 访问块 1：应命中（序列1访问过 16~31 的块 1）
- 访问块 4096：第一次 miss，紧接着第二次访问同块 4096 应 hit
- 访问块 0：应命中（序列1第一项就是块 0）
- 访问块 19：应命中（序列1最后一项就是块 19）

这样应当是 4/8 命中。

但题目观测是 3/8，说明在这 4 个“本该命中的点”里，**恰好有一个变成了 miss**。最合理、也最稳定能解释的就是：**访问块 0 时变成 miss（块 0 被挤掉了）**。

为什么块 0 会被挤掉？看块 0 所在的 set 在再次访问块 0 之前，经历了什么。

注意一个关键事实：无论 cache 是 4KB 还是 8KB，只要相联度 A ∈ {2,4,8}，set 数 S（`S =cache size / block size / A`） 都会是 32/64/128/256 之一，而

- 4096、8192、16384 都是 4096 的倍数
- 对 S=32/64/128/256，都有 4096 mod S = 0
- 同样 0 mod S = 0

所以这四个块：0、4096、8192、16384 **一定映射到同一个 set（set 0）**（不依赖你选 4KB/8KB，也不依赖你选 2/4/8-way）。

现在看序列2里，块 0 的两次相关访问之间发生了什么：

- 在再次访问块 0（地址 8）之前，你已经把 **4096、8192、16384** 这三个新块都装进了 set 0
- 再加上原来 set 0 里就可能有块 0

也就是说：在 set 0 里出现过的不同块至少是 {0, 4096, 8192, 16384} 共 4 个。

如果相联度是：

- 8-way：set 0 可以容纳 8 个不同块 ⇒ 块 0 不会被挤掉 ⇒ 访问 8 应该 hit ⇒ 命中数会回到 4/8（与观测不符）
- 4-way：set 0 正好能容纳 4 个不同块 ⇒ 块 0 也不会被挤掉（仍可同时容纳 0、4096、8192、16384）⇒ 访问 8 仍应 hit ⇒ 仍是 4/8（与观测不符）
- 2-way：set 0 只能容纳 2 个不同块 ⇒ 当你依次装入 4096、8192、16384 时，set 0 必然发生替换，把更早的块挤掉；块 0 很容易在这过程中被挤出 ⇒ 访问 8 变成 miss ⇒ 命中从 4 变成 3（与观测一致）

因此，相联度只能是 **2-way**。

**答案：相联度只能是 2-way**

------

第三题：

只用“命中率”反推 **替换策略**。核心思路是：

- **替换策略只在同一个 set 被塞满、又来一个映射到同 set 的新块时才会生效**
- 所以要找：哪些访问一定映射到同一个 set，并且在后续访问中用 hit/miss 去区分 LRU 与 FIFO



必须先写出“2-way + 16B block”的地址映射关系（不需要先知道 cache 是 4KB 还是 8KB）

设

- B = 16（块大小）
- A = 2（相联度）
- Cache 行数 L = C/B（C 是 cache 总大小）
- set 数 S = L/A = C/(B·A)

对任意地址 addr：

- block_number = ⌊addr / 16⌋
- set_index = block_number mod S
- offset = addr mod 16

替换只发生在：**同一个 set 里已经有 2 个不同 block（因为 2-way），又来了第三个不同 block 也要进这个 set**。



关键观察：块号 0、4096、8192、16384 一定在同一个 set（不依赖 cache 是 4KB 还是 8KB）

你给的序列里出现了块号：

- 0（地址 0、8、4 都属于块 0）
- 4096（地址 65536、65537）
- 8192（地址 131072）
- 16384（地址 262144、262145）

原因：S 一定是 128 或 256（因为 C 是 4KB 或 8KB，B=16，A=2）

- 若 C=4KB：L=4096/16=256，S=256/2=128
- 若 C=8KB：L=8192/16=512，S=512/2=256

而 4096、8192、16384 都是 256 的倍数，也是 128 的倍数，所以：

- 0 mod 128 = 0，4096 mod 128 = 0，8192 mod 128 = 0，16384 mod 128 = 0
- 0 mod 256 = 0，4096 mod 256 = 0，8192 mod 256 = 0，16384 mod 256 = 0

结论：无论 cache 4KB 还是 8KB，这四个块都映射到 **set 0**，并且会在这个 set 内触发替换。



由上：在 **序列2结束后**（序列3开始前）可以确定：

- set0 = {16384, 0} （这一点对 LRU 和 FIFO 都成立）

------

用序列 3 的命中率 2/3 区分 FIFO vs LRU（这是推替换策略的核心）

序列3地址：262145, 65536, 4
对应块号：

- 262145 → 16384
- 65536 → 4096
- 4 → 0

序列3开始前我们已知：set0 = {16384, 0}

现在逐步分析，并分别假设“替换策略是 LRU”或“替换策略是 FIFO”。

Step1：访问 16384

- 16384 在 set0 里 → **hit**
- hit 之后：
  - LRU 会更新“最近使用”信息：16384 变成最近用
  - FIFO 不会因为 hit 改变进入顺序（FIFO只看“谁先进入”）

Step2：访问 4096

- 4096 不在 set0 → **miss**，需要装入 set0
- set0 只有两路，必须替换 {16384, 0} 其中一个

现在分叉：

情况 A：LRU

- LRU 规则：替换“最近最久没被用过”的那块
- Step1 刚访问过 16384，所以 16384 是“最近用过”
- 0 上一次使用是在序列2的 Step6（更久之前），所以 LRU 会替换 **0**
- 装入 4096 后：set0 = {16384, 4096}

Step3：访问 0（块 0）

- 在 LRU 情况下，0 已经被 Step2 换掉了 → **miss**
- 那么序列3命中应该是：hit, miss, miss → 1/3
- 但题目观测是 2/3
- 所以 **不可能是 LRU**

情况 B：FIFO

- FIFO 规则：替换“最早进入 set 的那块”（不看最近用没用）
- 在 set0 = {16384, 0} 中：
  - 16384 是序列2 Step5 进入 set0
  - 0 是序列2 Step6 才重新进入 set0
  - 所以更早进入的是 **16384**
- FIFO 会替换 **16384**
- 装入 4096 后：set0 = {0, 4096}

Step3：访问 0

- 0 仍在 set0 → **hit**
- 序列3命中：hit, miss, hit → 2/3
- 与观测完全一致

**Answer：替换策略 = FIFO**

---

Cache size?

已知：block size = 16B、2-way、FIFO 之后，**cache size（4KB vs 8KB）**的推法核心是：
cache size 决定 **总行数 L** 和 **set 数 S**，从而决定哪些块会落到同一个 set、会不会发生额外冲突替换。

先把两种 cache size 对应的结构算出来（2-way 已确定）

- 4KB：
  - 行数 (L = 4096 / 16 = 256) lines
  - set 数 (S = L / 2 = 128) sets
- 8KB：
  - 行数 (L = 8192 / 16 = 512) lines
  - set 数 (S = L / 2 = 256) sets

![image-20260102220126700](C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260102220126700.png)

所以 **4KB vs 8KB 的区别**只体现在：

- 4KB 用 (S=128) 做 mod
- 8KB 用 (S=256) 做 mod
  如果某些块在 mod128 下会撞到同一个 set、但在 mod256 下不会撞，那两种 cache size 的 hit/miss 就会不同，你才能用观测数据区分。



把题目里出现的 block_number 列出来（只看“不同块”）

序列1用到：

- 0, 1, 64, 15, 68, 19

序列2新增：

- 4096, 8192, 16384, 66（以及 1、0、19 重复）

序列3：

- 16384, 4096, 0（重复）

不同块总计大约就这些：
({0,1,15,19,64,66,68,4096,8192,16384})（10 个左右）

逐个看（只看 block_number mod 128 / mod 256）：

- 小于 128 的块：0,1,15,19,64,66,68
  - 对它们来说：
    - mod 128 结果就是它自己
    - mod 256 结果也就是它自己
  - 所以 set_index 完全相同
- 大块：4096, 8192, 16384
  - 4096 是 256 的倍数，也是 128 的倍数
  - 8192 是 256 的倍数，也是 128 的倍数
  - 16384 是 256 的倍数，也是 128 的倍数
  - 所以它们在两种情况下都是：
    - mod128 = 0
    - mod256 = 0
  - 都落在 set 0

结论：
这组 trace 里，所有块落到哪个 set，在 4KB（128 sets）和 8KB（256 sets）下**完全一样**。
因此你之前用序列2/3 推出来的“set0 发生替换 → 2-way + FIFO”，在两种 cache size 下都同样成立。

**Answer： cache size 是 4KB 还是 8KB 无法确定**

---

## Chapter 3 虚拟内存

### 虚拟内存相关概念

**虚拟内存（Virtual Memory, VM）**

- 应用产生的地址为虚拟地址（VAs）
  - 每个进程以为自己有 (2^N) 个字节的地址空间
- 内存访问使用物理地址（PAs）
- 虚拟地址到物理地址的转换以 page 为单位（粗粒度）
- 操作系统（OS）负责虚拟地址到物理地址的映射（页分配）

解释

- CPU 所访问的地址都是虚拟地址，而想要写数据/读数据都需要先经过VA->PA的映射；
- 程序→虚拟地址；硬件/操作系统→把虚拟地址转换成物理地址；内存系统最终用物理地址读写数据。
- “以 page 为单位”表示映射不是按字节逐个建立，而是按固定大小的页（page）为基本单位建立映射关系。一个页内的所有字节共享同一个“虚拟页号→物理页号”的映射。
- CPU 每执行一条指令，至少要取指令本身；很多指令还要读/写数据。这些取指令、读写数据都是“内存访问”，都必须把虚拟地址先翻译成物理地址才能真正访问。

---

**程序使用虚拟地址（VA）**

- 虚拟地址位数为 N bits（指针大小，现在通常为 64 bits）

解释

- 虚拟地址是程序“看到并使用”的地址。编译器生成的指针、数组下标换算出的地址、函数返回的指针等，都是虚拟地址。
- “N bits”是虚拟地址的位宽，也常等同于该架构下指针的位宽。位宽决定虚拟地址空间的上限：可表达的虚拟地址范围最大是 (2^N) 字节。

---

**内存使用物理地址（PA）**

- 物理地址位数为 M bits，M 通常小于 N
- (2^M) 表示能支持最大的物理内存
- 现代计算机一般采用 48 bits

解释

- 物理地址是内存系统真正使用的地址，用来定位内存条上的具体字节位置。
- M 通常小于 N：虚拟地址空间往往设计得比物理内存大得多，允许每个进程看到很大的地址范围，而实际机器内存容量由 PA 位数与实现决定。

---

**虚拟地址→物理地址转换以 page 为粒度**

- 映射不需要保持物理页连续
- 虚拟页可能没有映射到任何物理页
- 没有映射的虚拟页可能在磁盘上（换出去了）或者还未被使用过

解释

- 页（page）是虚拟内存管理的基本分配/映射单位（例如 4KB、2MB 等）。
- “物理页不需要连续”表示：一个进程的连续虚拟地址范围，在物理内存中可以对应到不连续的物理页帧。连续性对程序可见（虚拟地址连续），对物理内存不要求连续。
- “虚拟页可能没有映射”表示：某些虚拟页当前没有对应的物理页帧。原因包括：
  - 该页从未被访问/分配（未使用）
  - 该页内容被换出到磁盘（swap）
  - 该页对应的是非法地址区间（访问应当触发异常）
- 当 CPU 访问一个“未映射/无效”的虚拟页时，硬件会检测到页表项状态不允许访问，从而触发异常，由操作系统决定如何处理（例如分配新页、从磁盘换入，或直接终止进程）。

------

### 虚拟内存好处

好处1：**多程序隔离**

- 多程序之间隔离
  - 每个程序以为自己有 (2^N) 大小的内存空间
  - 防止程序互相访问彼此的内存空间
    - 无法知道其他程序的物理地址！

解释

- 隔离的目标是：一个进程不能随意读写另一个进程的数据。
- 实现手段是“每个进程有自己的虚拟地址空间 + 操作系统控制映射”：即使两个进程用了相同的虚拟地址数值（例如都访问 0x1000），也会被映射到不同的物理页，或访问权限不同。
- “不知道其他程序的物理地址”强调：程序层面只看到虚拟地址；物理地址与映射关系由操作系统管理，普通进程不能直接获取并使用别的进程的物理地址来绕过隔离。

------

好处2：**安全保护（权限位）**

- 安全保护
  - 每个页有读/写/执行的权限（OS 设置）
  - 由硬件保证

解释

- 每个虚拟页在页表项中都会带有权限信息（常见是 R/W/X：可读、可写、可执行，可能还有用户态/内核态等属性）。
- 操作系统在建立映射时设置这些权限。
- “由硬件保证”表示：CPU 在做地址翻译与访问时会检查权限位。如果指令尝试做不允许的操作（例如向只读页写入），硬件会触发异常，进入OS处理流程。

------

好处3：**进程之间通信（共享映射、mmap）**

- 进程之间通信
  - 将同一个物理页映射到多个虚拟地址空间
  - 或者通过 UNIX mmap() 共享文件

解释

- 不同进程的页表可以把各自某个虚拟页号映射到同一个物理页帧。这样一个进程写入该物理页，另一个进程读到的就是同一份数据，从而实现共享内存通信。
- “mmap() 共享文件”表示：操作系统可以把文件内容映射到进程的虚拟地址空间。多个进程 mmap 同一个文件（或同一个共享内存对象）时，可以共享同一组物理页缓存，实现进程间共享数据与高效 I/O。

------

### 地址转换

- 虚地址到物理地址的映射称为地址转换
  - 将虚地址分成虚拟页号（VPN）& 页内偏移（page offset）
  - 将虚拟页号转换为物理页号（PPN）
  - 页内偏移不需要转换
- 例如
  - 页大小为 64KB → 16-bit 页内偏移（64K = 2 ^16 B）
  - 32-bit 计算机 → 32-bit 虚拟地址 → 16-bit 虚拟页号
  - 最大 256MB 物理内存 → 28-bit 物理地址 （256 M = 2 ^ 28 ）→ 12-bit 物理页号（28 - 16 = 12）

解释

- 地址转换的核心是“页号替换、页内偏移保留”：
  - **虚拟地址 = VPN（决定是哪一页） + page offset（页内具体字节位置）**
  - 通过查映射关系，把 VPN 换成 PPN
  - **page offset 原样带到物理地址中，因为同一页内的第几个字节在虚拟页与物理页中意义一致**
- **页大小决定 offset 的位数：页大小为 (2^k) 字节，则 page offset 占 k 位**。64KB = (2^{16})，所以 offset 是 16 位。
- VPN 位数 = 虚拟地址位数 − offset 位数。
- PPN 位数 = 物理地址位数 − offset 位数。

------

**VPN（Virtual Page Number，虚拟页号）与 POFS（Page Offset，页内偏移）**

提取文字

- virtual address[31:0] = VPN[31:16] + POFS[15:0]
- physical address[27:0] = PPN[27:16] + POFS[15:0]

图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103111341403.png" alt="image-20260103111341403" style="zoom:67%;" />

解释

- VPN 用于定位“这是第几个虚拟页”，它是查页表（或多级页表）时的索引依据。
- POFS 是页内偏移，表示在该页内的具体字节位置。它不会参与“虚拟页→物理页”的映射选择，只在最终形成物理地址时拼接回去。
- 图中例子表达的是：虚拟地址 32 位、页大小 64KB（offset 16 位），因此 VPN 是高 16 位；物理地址 28 位，因此 PPN 是高 12 位；offset 仍然是低 16 位。

------

**PPN（Physical Page Number，物理页号）**

- 将虚拟页号转换为物理页号（PPN）
- 页内偏移不需要转换

解释

- PPN 表示物理内存中的“第几个物理页帧”。物理页帧大小与页大小一致。
- PPN 是页表查找的结果：给定 VPN，页表项中记录了对应的 PPN（或记录该页在磁盘上的位置、或标记无效）。
- 形成物理地址时，用 PPN 替换 VPN，再拼接同一个 offset，即得到最终访问内存所需的物理地址。

------

**页表（Page Table, PT）**

- 地址如何转换？
  - 软硬件协同完成
- 每个进程有一个页表
  - **由操作系统维护的数据结构**
  - 将虚拟页映射到物理页或磁盘地址
    - 如果页从未被访问过，虚拟页对应的项为空
  - 地址翻译就是查页表

解释

- **页表是“每个进程自己的映射表”**：输入 VPN，输出与该虚拟页对应的状态与位置。
- 页表项可能指向：
  - 某个物理页帧（表示该页在内存中，访问可直接进行）
  - 磁盘上的 swap 位置（表示该页被换出，访问需换入）
  - 无效/空（表示未分配或不允许访问）
- **“地址翻译就是查页表”表示：VPN 用作索引，找到对应页表项后，得到 PPN 与权限等信息，才能继续访存**。

------

**页表项（Page Table Entry, PTE）**

- 页表最下层保存页表项（PTEs）

解释

- PTE 是页表中的一个条目，用来描述某个虚拟页的状态。典型字段包括：
  - 物理页号 PPN
  - valid/present 位（是否在内存、映射是否有效）
  - 权限位（读/写/执行、用户/内核等）
- 在多级页表中，只有最底层表的条目才是“真正的 PTE”（直接给出最终 PPN 与权限）；上层表条目更多是指向下一级页表的地址（指针）。

------

#### 单级页表例子

**页表起始地址（页表基址）与示例地址**

- Example：内存访问地址为 0xFFA8AFBA
- 页表起始地址 0xFFFF87F8

如图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103112407530.png" alt="image-20260103112407530" style="zoom:67%;" />

解释

- “页表起始地址”表示：当前进程页表在内存中的起点位置（硬件需要知道从哪里开始查表）。我们OS中学的是用页表基址寄存器（SATP）来保存页表的基本地址。
- 给定一个虚拟地址（示例 0xFFA8AFBA），处理过程是：
  - 先按页大小把它切分成 VPN 与 offset
  - 用 VPN 计算页表项所在位置：页表基址 + VPN × PTE 大小（这是单级页表的基本形式）
  - 读取该 PTE 得到 PPN 与权限
  - 拼接 PPN 与 offset 得到物理地址，然后内存系统用该物理地址完成读写
- 多级页表时，这个“找到 PTE 的过程”会变成逐级索引与读取下一级页表地址，最终才读到最底层 PTE。

------

**页表大小计算**

- 以下计算机的页表大小如何计算？
  - 32-bit 计算机
  - 4B 页表项大小（PTEs）
  - 4KB 页大小
- VPN[20 bits]，POFS[12 bits]
- 32-bit 计算机 → 32-bit 虚拟地址 → (2^{32}) = 4GB 虚拟内存
- 4GB 虚拟内存 / 4KB 页大小 → 1M 虚拟页
- 1M 虚拟页 × 4 Bytes/每页表项 → 页表大小——4MB
- **页表可能很大**

解释

- **页表大小取决于“虚拟页的数量 × 每个页表项大小”。**
- **虚拟页数量 = 虚拟地址空间大小 / 页大小。**
  - 32 位虚拟地址空间是 4GB
  - 页大小 4KB，则虚拟页数是 (4GB / 4KB = 2^{32} / 2^{12} = 2^{20} = 1M)
- 每页一个 PTE，每个 PTE 4B，则页表大小 (1M * 4B = 4MB)。这是“单级页表”的典型结果。
- 页变大（例如 64KB）会让 offset 位数变多、VPN 位数变少，从而虚拟页数量减少，页表条目变少，页表整体变小。
- 64 位虚拟地址空间极大，如果仍按“单级页表、每页一个 PTE”去覆盖整个空间，页表会大到不可接受，因此现代系统会使用多级页表、按需分配页表页、以及其他机制来控制大小。

------

#### 多级页表

**多级页表**

- 将页表变为树形结构
- 最下层保存页表项（PTEs）
- 上层页表保存到下层的指针
- 虚拟页号的不同部分用于索引不同的层
- 20-bit 虚拟页号
  - 高位 10 bits 索引第 1 层
  - 低位 10 bits 索引第 2 层
  - 实际中，通常大于 2 层

图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103112743423.png" alt="image-20260103112743423" style="zoom:50%;" />

解释

- 多级页表把原来“一个巨大数组式页表”拆成多层：
  - 顶层表不直接给出 PPN，而是给出“下一层页表在哪里”
  - **最底层表才给出最终 PTE（包含 PPN** 与权限等）
- **VPN 被切成若干段，每一段作为一层的索引**：
  - 先用高位段在第 1 层找到对应的“下层表地址”
  - 再用下一段在第 2 层继续索引
  - 直到最底层得到最终 PTE
- **多级页表的直接好处是“按需分配”**：只有当某一段虚拟地址范围被使用时，对应的下层页表页才需要存在；未使用的大范围虚拟地址不需要为其分配整片页表空间。
- **层数增加会让一次地址翻译需要更多次内存读取（页表遍历）**，因此实际系统通常还会配合硬件缓存来降低平均翻译开销。

------

**多级地址转换**

- Example：内存访问地址 0xFFA8AFBA
- 页表起始地址 0xFFFF87F8
- 图中给出了第 1 层表项指向第 2 层页表的位置，以及最终 PTE 给出物理页号
- Physical Address = Physical Page Number + Page Offset

如图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103112911865.png" alt="image-20260103112911865" style="zoom:67%;" />

解释

- 多级地址转换的过程本质是“逐级查表”：
  - 第 1 层：用 VPN 的高位片段索引顶层页表，读出一个指向下层页表的地址
  - 第 2 层：用 VPN 的低位片段索引第二层页表，读出最终 PTE（含 PPN 与权限）
  - 拼接：把 PPN 与原 offset 拼接成物理地址
- 这种“逐级查表”会产生额外的内存访问次数：每一层至少要读一次对应表项，层数越多，遍历开销越大。
- 因此在性能设计上，需要把“页表占用空间”和“翻译访问次数”一起考虑，并通常依赖硬件缓存来让大多数翻译不必每次都做完整遍历。

------

### 大页优缺点

- 很多 ISAs 支持多种页大小
  - x86：4KB，2MB，1GB
- 大页的优缺点
  - 减少页表空间
    - 表项变少了
  - 页表层数更少
    - 查找速度快
  - 页内空间浪费更严重
    - 分配 2MB 大小的页但只用了 5KB
  - 实现更复杂
    - OS 会寻求其他措施提高 page 利用率

解释

- 大页带来的页表收益：页更大 → 页数更少 → PTE 数更少 → 页表空间下降；多级页表中也可能减少需要分配的下层页表页数量，并可能减少遍历层数或减少遍历频率。
- 大页的主要代价是页内浪费（内部碎片）：当程序只用到一个大页中的很小一部分时，其余部分仍被当作已分配的页内容占用物理内存资源。
- “实现更复杂”通常体现在：
  - 操作系统要维护多种页大小的映射与权限
  - 需要选择何时使用大页、何时拆分/合并页
  - 需要在减少页表/提升覆盖与降低内部碎片之间做权衡

---

### TLB(key)

#### TLB基本概念

概念上，如果**每一次内存访问（包括访问 cache）之前都去查页表（page table）**，会导致每次访问都变成“先访页表、再访数据”，效率很低。所以实际上，用 **TLB** 把“VPN→PPN 的转换结果”缓存起来，只有 **TLB 未命中**时才需要真的去访问页表。

**TLB（Translation Lookaside Buffer）**
TLB 本质上是一个专门缓存“地址转换结果”的小型 cache：

- TLB 的每个表项通常包含：VPN（用于匹配的 tag）、PPN（转换后的结果）、以及权限/有效位等信息（用于保护检查）。
- TLB 表项数量较少（16-64），但通常**关联度很高**（>4 或者全相联），目的是降低冲突未命中。
- **TLB 能利用“页表访问的时间局部性”**：程序往往在一段时间内反复访问同一批页，因此同一批 VPN→PPN 映射会被频繁复用。
  当 TLB 未命中时，会触发“未命中处理程序”，去**遍历页表**找到对应 PTE（页表项），再把新的映射写回 TLB。

TLB示意图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103115350766.png" alt="image-20260103115350766" style="zoom:67%;" />

TLB内部类似：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103120339681.png" alt="image-20260103120339681" style="zoom: 80%;" />

#### TLB & Cache

**TLB & Cache（为什么它们经常要配合考虑）**
很多体系里，普通数据/指令 cache 使用**物理地址**来做 index/tag（即“物理寻址 cache”）。这样做的直接后果是：

- 好处：不同进程的物理地址空间天然隔离；上下文切换时通常**不需要因为地址空间切换而 flush cache**；甚至可以让多个进程共享同一物理页，从而共享同一个 cache block（便于基于 cache 的进程间共享/通信）。
- 代价：即使 TLB 命中，仍然要先完成 VA→PA，再去访问物理寻址的 cache，因此在关键路径上可能多出额外延迟（讲义里直接写“慢：TLB 命中仍需要增加一个周期”这一类结论）。

同时 **TLB 自身用虚拟地址做 index/tag**：TLB 查的是 VA 里的 VPN，因此当切换进程（上下文切换）时，如果不区分不同进程的地址空间，TLB 里的旧映射可能会与新进程冲突，所以需要 flush。

#### TLB Miss

**TLB 未命中（TLB miss）**
TLB miss 的定义是：要做的地址转换结果**不在 TLB**里，但仍然**可以在页表里找到**（页表里有对应的 PTE）。讲义给了两类处理方式，特点是“都相对比较快”（相比缺页异常）。

- 基于硬件的方式
  硬件知道页表的组织方式，页表基地址等信息由寄存器提供；**TLB miss 时由硬件自动完成页表遍历并取回映射，然后更新 TLB**。
  - 好处：**避免进入操作系统的 miss handler**，从而避免那种由 OS 介入带来的流水线清空等额外代价；
  - 缺点：页表格式需要“硬编码/固定”。
- 基于软件的方式（software-managed TLB miss）
  TLB miss 时触发异常/中断进入 OS，由 **OS 用一小段例程（例如十条指令量级）去遍历页表并更新 TLB**。
  - 优点：页表格式可以更灵活；
  - 缺点：引入 OS 介入带来的开销（包括额外的内存访问与进入内核导致的流水线相关代价）。
- **当前基于硬件的方式更受欢迎。**

#### 缺页异常

**缺页异常（Page Faults）**
**缺页异常讨论的是更严重的一类情况：访问的页既不在 TLB，也无法在“当前可用的主存驻留集合”中直接得到**（讲义表述为：**访问页不在 TLB 和页表中**，并指出页可能在磁盘或尚未分配）。如果页根本没分配，属于非法访问，会触发 segmentation fault（段错误）。

发生缺页时，OS 需要完成一系列明确动作：

- 选择一个要被替换掉的物理页（需要页替换策略）。
- 如果被替换页是**脏页（dirty）**，必须先写回磁盘（因为磁盘上的副本不是最新的）。
- 从磁盘把缺失页读入内存。
- 更新页表；刷新/失效相关 TLB 表项；然后让原本失败的内存访问重试。

这一步的关键特征是**延迟极长（~10ms 量级）**，因此 OS 往往会在等待 I/O 时调度其他任务运行。

---

**虚地址cache引起的Aliasing** 略，Lys说不考

---

### TLB & Cache并行访问

#### 定义
并行访问指的是：在一次 load/store 或取指过程中，CPU 同时启动两件事——用虚拟地址去查 TLB（做 VA→PA 的页号转换），同时也启动对 L1 Cache 的访问流程。这样做的目的不是改变命中率，而是减少“先等地址翻译完成、再访问 cache”的串行等待时间，因为 cache 的数据阵列读取、tag 比较、TLB 查找本质上都是可以重叠的操作。

并行的关键限制在于：Cache 用到的 **index** 必须能直接从虚拟地址中得到，并且在翻译前后保持一致，否则你在翻译完成前无法确定要访问 cache 的哪一个 set。

**Cache 地址构成：tag + index + offset**
Cache 访问时，地址会被切分成三部分。
tag 用来做“是不是同一块”的匹配；index 用来选择 set（或直接映射时选择 line）；offset 用来选择一个 cache block 内的具体字节/字。访问流程上通常是：先用 index 定位到一个 set（组相联就会同时读出该 set 的所有 ways 的 tag/data），再用 tag 做并行比较确定命中哪一路，最后用 offset 取到所需的字节/字。这里的关键点是：**index 决定你要读 cache 的哪一组数据阵列**，因此如果 index 不能在翻译前确定，就无法做到 TLB 和 cache 的真正并行。

**TLB 地址构成：VPN + page offset**
**TLB 查找使用的是虚拟地址的“虚拟页号 VPN”部分作为匹配键**，page offset 不参与页号映射（它在页内定位字节/字，翻译后保持不变）。TLB 命中时直接得到 PPN（物理页号），随后把 PPN 与原来的 page offset 拼接得到物理地址。TLB 不关心 cache block 的 offset，它关心的是页级别的划分：VPN 决定是哪一页，page offset 决定页内位置。

#### 并行访问成立条件

**并行访问成立的条件： (cache size) / (associativity) ≤ page size**

我们先解释下为什么这个式子代表着并行访问的成立：

VA = VPN + Page offset 

Page offset = log2（Page size）

而对于cache:

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103123617286.png" alt="image-20260103123617286" style="zoom:80%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103123643899.png" alt="image-20260103123643899" style="zoom:80%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103123657170.png" alt="image-20260103123657170" style="zoom:80%;" />

---

再解释一下？

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103123733023.png" alt="image-20260103123733023" style="zoom:80%;" />

---

#### 若不满足并行条件（index太长）？

**如果 index 太长会发生什么：Index bits 在 VA 和 PA 中可能不同**
当 (cache size)/(associativity) > page size 时，set 数太多导致 index 位数增加，index 的高位会取到 VPN 的部分。由于 VPN 会被翻译成 PPN，而 PPN 与 VPN 不同，因此：

- 同一个物理页（同一个 PPN）对应的不同虚拟映射（不同 VPN）可能在虚拟地址上表现为不同的 index
- 翻译前用 VA 取到的 index，和翻译后用 PA 应该对应的 index 不一致
  这会让“先按 VA 访问 cache 再做翻译”的做法出现一致性问题。

**Aliasing（同一物理地址在 cache 中出现多个副本）**
Aliasing 指的是：存在两个不同虚拟地址 (V1, V2)，它们通过页表映射到同一个物理页（也就是翻译后物理地址相同），但因为它们的虚拟 index bits 不同，导致 cache 把它们放到了不同的 set/line 位置，于是同一份物理数据在 cache 里出现两个不同副本。后果是：

- 读可能读到“旧副本”（因为写只更新了其中一个副本）
- 写回/一致性变得复杂（你必须保证所有别名位置同步更新或失效）
  这不是普通的 conflict miss 问题，而是“同一物理地址被缓存了多份”的正确性风险。

#### 解决方案（VIPT）

**解决方案：Virtual-Index Physical-Tag（VIPT）**
VIPT 的核心目标是同时满足两点：

- **索引用虚拟地址（Virtual-Index）**：让 cache 能在地址翻译未完成前就启动 set 选择与数据阵列读取，实现与 TLB 并行
- **tag 用物理地址（Physical-Tag）**：用翻译后的物理 tag 来判定命中，保证同一物理块只有一个正确的身份，避免 aliasing 带来的“同物理不同副本”的错误命中

在讲义的方案里，为了处理“index 太长”的情况，会把 index 拆成两部分：

- 低位的 (L − a) bits 直接来自虚拟地址中不变的那部分（通常落在 page offset 内）
- 额外的 a bits 会导致需要同时探测更多候选位置，于是 cache 会并行读出 (2^a) 个候选 block（等价于扩大并行查找范围）
  TLB 并行完成翻译后，得到物理地址的 tag，再把这个物理 tag 与这 (2^a) 个候选 block 各自的 tag 同时比较，只有匹配的那一路才算真正命中。这样即使虚拟 index 与物理 index 的关系复杂，也能在“并行启动访问”的同时，用物理 tag 最终裁决正确的命中位置，从而规避 aliasing。

---

## VM 期末题目（大题其一，Hw3）

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103124719711.png" alt="image-20260103124719711" style="zoom:80%;" />

**解答：**

**先把题目条件转成“能直接算”的量**

**虚拟地址 8-bit 意味着有多少虚拟页**
页大小是 16 bytes，所以页内偏移需要 ( log_2（16） = 4) 位。虚拟地址总共 8 位，因此虚拟页号（VPN）是高 4 位，一共 (2^4=16) 个虚拟页，编号 (0 - 15)。题目给的访问序列已经是“访问哪些虚拟页”，所以不需要再从地址里拆 VPN/offset。

**物理内存 128 bytes、物理页 16 bytes 意味着有多少物理页框**
(128/16=8) 个物理页框（PPN 0~7）。其中 PPN7 放的是 Page Table，不能拿来装普通数据页，所以真正可用于装虚拟页内容的物理页框是 PPN0~6 共 7 个。

**初始内存布局（题目已给）对应的页表状态是什么**
PPN1 装 VP13 ⇒ 页表里 (VP13 ---> PPN1)
PPN2 装 VP5 ⇒ 页表里 (VP5 ---> PPN2)
PPN3 装 VP2 ⇒ 页表里 (VP2 ---> PPN3)
PPN5 装 VP0 ⇒ 页表里 (VP0 ---> PPN5)
PPN0、4、6 为空 ⇒ 这三个物理页框是“可直接分配”的空闲页框

**TLB 容量 3 项 + LRU 的规则怎么用**
TLB 里存的是 (VPN ---> PPN)。
命中（hit）意味着这次访问的 VPN 在 TLB 里，直接得到 PPN。
未命中（miss）意味着 VPN 不在 TLB，需要去查页表：

- 如果页表里有映射（页在内存里），这是“TLB miss but no page fault”，查到 PPN 后把该项填回 TLB（必要时按 LRU 逐出一项）
- 如果页表里没有映射（页不在内存里），这是“page fault”，需要先找一个物理页框装入该页，再更新页表，然后把映射写入 TLB
  题目说 TLB 用 LRU 替换，这里采用标准 LRU：每次访问命中/填入都会把对应项更新为“最近使用”，逐出的是“最久未使用”。

------

**每一步怎么走：这次访问是 TLB hit 还是 miss？会不会 page fault？TLB/页表怎么变**

下面表格里，**TLB 状态按“左边最久未用，右边最近使用”写出**；“装入/替换的物理页框”只在发生 page fault 时出现；“逐出的物理页（发生 page fault 且内存满时）”表示物理内存页替换。

| 步骤 | 访问的 VPN | TLB  | Page Fault | 访问后该 VPN 在哪个 PPN                                      | page fault 时逐出的物理页 | TLB（VPN->PPN）                                  |
| ---- | ---------- | ---- | ---------- | ------------------------------------------------------------ | ------------------------- | ------------------------------------------------ |
| 0    | null       | null | 否         | null                                                         | null                      | 开始填的TLB：0->5 , 2->3 , 13->1                 |
| 1    | 0          | hit  | 否         | 5                                                            |                           | 2→3，13→1，0→5                                   |
| 2    | 13         | hit  | 否         | 1                                                            |                           | 2→3，0→5，13→1                                   |
| 3    | 5          | miss | 否         | 2                                                            |                           | 0→5，13→1，5→2 （按照LRU规则把映射2→3换出去）    |
| 4    | 2          | miss | 否         | 3                                                            |                           | 13→1，5→2，2→3（按照LRU规则把映射0->5换出去）    |
| 5    | 14         | miss | 是         | 0（页表缺乏对于VPN 14 的映射，从上到下选择映射到 PPN 0）     |                           | 5→2，2→3，14→0（按照LRU规则把映射13->1换出去）   |
| 6    | 14         | hit  | 否         | 0                                                            |                           | 5→2，2→3，14→0                                   |
| 7    | 13         | miss | 否         | 1                                                            |                           | 2→3，14→0，13→1（按照LRU规则把映射5->2换出去）   |
| 8    | 6          | miss | 是         | 4（页表缺乏对于VPN 6 的映射，从上到下选择映射到 PPN 4）      |                           | 14→0，13→1，6→4（按照LRU规则把映射2->3换出去）   |
| 9    | 6          | hit  | 否         | 4                                                            |                           | 14→0，13→1，6→4                                  |
| 10   | 13         | hit  | 否         | 1                                                            |                           | 14→0，6→4，13→1                                  |
| 11   | 15         | miss | 是         | 6 （页表缺乏对于VPN 15 的映射，从上到下选择映射到 PPN 6）    |                           | 6→4，13→1，15→6（按照LRU规则把映射14->0换出去）  |
| 12   | 14         | miss | 否         | 0                                                            |                           | 13→1，15→6，14→0（按照LRU规则把映射6->4换出去）  |
| 13   | 15         | hit  | 否         | 6                                                            |                           | 13→1，14→0，15→6                                 |
| 14   | 13         | miss | 否         | 1                                                            |                           | 14→0，15→6，13→1                                 |
| 15   | 4          | miss | 是         | 5 （页表缺乏对于VPN 15 的映射，但现在页表已经满了，我们还是按照LRU规则，VPN0访问间隔最久，选择破坏VPN0->PPN5的映射，改为VPN4->PPN5的映射） | 逐出 VP0@PPN5             | 15→6，13→1，4→5（按照LRU规则把映射14->0换出去）  |
| 16   | 3          | miss | 是         | 2（页表缺乏对于VPN 15 的映射，但现在页表已经满了，我们还是按照LRU规则，VPN5访问间隔最久，选择破坏 VPN5->PPN2 的映射，改为VPN3->PPN2 的映射） | 逐出 VP5@PPN2             | 13→1，4→5，3→2（按照LRU规则把映射 15->6 换出去） |

------

**(a) 这串访问的 TLB 命中率怎么算**

**要数的“命中”到底是什么**
命中只看“访问的 VPN 是否已经在 TLB 里”。即使页在内存里（页表有映射），但 TLB 没有该项，仍然算 TLB miss。

**哪些步骤是 hit**
从表里取 TLB=hit 的步骤：1、2、6、9、10、13、（其余均 miss）
命中次数是 7 次，总访问次数是 16 次

`命中率 = 7 / 16 = 43.75 %`

------

**(b) 全部访问结束后，TLB 里的内容是什么**

**最后一次访问结束后的 TLB 状态是什么**
看第 16 步结束时的 TLB为：

- (VPN13 --> PPN1)
- (VPN4 --> PPN5)
- (VPN3 --> PPN2)

------

**(c) 全部访问结束后，页表（VPN→PPN 映射）是什么**

**哪些页一定在内存里，哪些被换出**
初始在内存里的有 VP0、VP2、VP5、VP13；访问过程中发生 page fault 装入了 VP14、VP6、VP15、VP4、VP3。
由于可用物理页框 PPN0/4/6 只有 3 个空闲，后续再出现新页（VP4、VP3）时需要物理页替换。按“最久未被访问的物理页”逐出：先逐出 VP0（它最后一次访问是第 1 步），再逐出 VP5（它最后一次访问是第 3 步），因此最终 VP0、VP5 不在内存里。

**最终物理内存中各物理页框装的虚拟页**

- PPN0：VP14
- PPN1：VP13
- PPN2：VP3
- PPN3：VP2
- PPN4：VP6
- PPN5：VP4
- PPN6：VP15
- PPN7：Page Table

---

## Chapter 4 复杂流水线

### 乱序执行（Tomasulo算法）

**乱序执行**
乱序执行也叫**指令动态调度**：和“编译器提前把顺序排好”的静态调度不同，它是**硬件在程序运行时**根据“资源是否空闲、操作数是否就绪”等实际情况，决定某些指令能不能先去执行。

乱序执行的基本流水是：指令译码后不立刻进执行单元，而是先进入一个能暂存指令的结构（保留站）；然后每个周期检查这些指令的源操作数是否就绪；若就绪且有空闲执行单元，就把指令送去执行；执行完成后，从保留站移除。

乱序执行的作用是：当某条指令因为执行延迟很长（例如除法、访存未命中）而占着顺序流水时，硬件可以让**后面那些不依赖它结果的指令**先执行，从而减少执行单元空转，让“无关的指令”不被“慢指令”拖住。

------

**乱序执行需要满足的条件（硬件要解决什么问题）**
要让“后面的指令先执行”仍然保证结果正确，硬件必须做到下面这些事：

- **建立数据生产者/消费者的关联关系（谁在等谁的结果）**
  硬件需要知道：某条指令读的寄存器值，究竟是“已经在寄存器里可用”，还是“要等某条还没完成的指令产生”。这就是后面会出现的 **tag（标签）/寄存器重命名** 的意义：把“将来会产生这个值的来源”记录下来。

- **缓存操作数还没准备好的指令**
  因为顺序执行时，遇到操作数没好只能停；乱序执行要继续推进，就必须把“暂时不能执行的指令”放在某个地方等待，这个地方就是**保留站**。

- **检测操作数是否就绪，并在就绪时通知相关指令**
  硬件要持续检测保留站里每条指令的源操作数状态；**当某条指令把结果算出来，需要把“这个结果对应哪个 tag”广播出去**；所有在等待的指令把自己记录的 tag 和广播的 tag 做比较，相等就表示自己等的那个操作数已就绪，可以把值拿到手。

- **当操作数都就绪后，完成唤醒与选择并送入执行单元**
  当某条指令的所有源操作数都就绪，就进入“可发送”的状态；但同一周期可能有多条指令同时就绪，而执行单元数量有限，所以硬件还需要做“选择”，挑出能发射到执行单元的指令。

------

**Tomasulo 算法（经典的乱序执行硬件实现思路）**
Tomasulo 的关键点是：用**保留站**缓存指令、用**寄存器重命名**解决名字相关（WAW/WAR两种假相关），并配合**总线广播 tag+结果**实现依赖跟踪与唤醒，从而支持乱序发送/执行。

**核心结构：保留站**
每个计算/执行单元旁边配一个保留站（可以理解为与该执行单元配套的一组表项/物理寄存器），用于存放“已经译码、但还不能立刻执行”的指令及其操作数信息。

**寄存器重命名：消除 WAW 和 WAR**
重命名的具体做法是：**把“寄存器编号”映射成“保留站表项编号（保留站ID）**”。当某条指令将写某个寄存器时，不再只用寄存器名来表示“未来的值”，而是用一个唯一的 tag（通常就是保留站ID或等价编号）表示“未来由谁产生”。这样就能避免：

- WAW：两条指令都写同一个寄存器名导致先后写入次序混淆
- WAR：后面的写把前面的读“覆盖掉”这种名字相关
  因为读写依赖被转换成“等某个 tag 的结果”，不再单纯依赖寄存器名字。

**发射（Issue）阶段：进保留站，否则 stall**
当一条指令准备进入乱序系统：

- 如果对应类型的保留站还有空表项：把指令送入保留站，并完成寄存器重命名。
- 如果保留站满了：这条指令无法进入，只能停顿（stall）。

**等待与监听：操作数由 tag 驱动就绪**
保留站中的每条指令都会监听总线：当某个结果完成后，硬件会广播“tag + 数据”；如果保留站里某条指令发现广播的 tag 等于自己等待的 tag，就把数据抓取到自己的表项中，更新为“该操作数已就绪”。

**可发送（ready）与发送到执行单元**
当一条指令的所有源操作数都已就绪，它就变成“可发送”；随后调度逻辑把它发给对应的执行单元开始执行（执行单元是否空闲会影响能不能立刻发）。

**写回与广播：结果写寄存器、唤醒等待者、回收表项**
指令执行结束后：

- 结果会在总线上广播，并携带“产生该结果的保留站ID（tag）”。
- 寄存器文件连接在总线上，寄存器会多一列用于记录 tag（文档里用 st 表示）：如果某个寄存器当前 st 记录的 tag 与总线 tag 匹配，说明寄存器等的结果到了，就把值写入寄存器，同时清除 tag。
- 对应的保留站表项被回收（清除该指令信息），让后续指令能进入。

------

**Tomasulo 中几类关键字段（它们在硬件里分别表示什么）**

PPT中图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143235414.png" alt="image-20260103143235414" style="zoom:80%;" />

---

**Op（操作码）**
保留站表项里记录指令类型（加/减/乘/除/访存等），用于决定将来送到哪个执行单元以及执行控制信号。

**Vj、Vk（操作数的值）**
当源操作数已经就绪时，直接把数值放在 Vj/Vk 中；执行单元需要数据时可以直接读取这些值。

**Qj、Qk（将提供该操作数的保留站ID）**
当某个源操作数暂时不可用（它要等另一条还没完成的指令产生），就把“未来会产生它的来源”记录为一个 tag（通常是保留站ID）放在 Qj/Qk 中；当 Qj/Qk 为 0 时表示“该操作数已准备好”。这就是依赖跟踪的核心：等的不是寄存器名，而是某个 tag。

**Registers 里的 st（寄存器的 tag 状态）**
每个寄存器除了存 value，还要有一列 st：

- st 为空：寄存器当前值就是最新可用值，可以被读。
- st 非空：寄存器正在等待某个保留站ID的结果；此时读该寄存器的指令不会直接拿 value，而是把这个 st 作为自己的 Qj/Qk 记录下来，等待总线广播匹配的 tag。

**Bus（总线广播 tag + 结果）**
总线承担两件事：广播计算结果的数据；同时广播“产生该结果的保留站ID（tag）”。保留站与寄存器文件都在监听它：

- 保留站用它来把 Qj/Qk 变为就绪并填入 Vj/Vk
- 寄存器文件用它来判断 st 是否匹配并完成写回
  这使得“谁等谁”可以通过 tag 自动驱动完成，而不需要按程序顺序硬等。

---

例子：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143414959.png" alt="image-20260103143414959" style="zoom: 67%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143430452.png" alt="image-20260103143430452" style="zoom:67%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143448552.png" alt="image-20260103143448552" style="zoom:67%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143506192.png" alt="image-20260103143506192" style="zoom:67%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143523384.png" alt="image-20260103143523384" style="zoom:67%;" />

---

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260103143538473.png" alt="image-20260103143538473" style="zoom:67%;" />

---

### 精确异常（PPT没但好像会考）

#### 精确异常/非精确异常

**精确异常**
要求：异常发生时，体系结构状态必须等价于“按程序顺序执行到触发异常的那条指令为止”。也就是说，异常点之前的所有指令对寄存器/内存的修改都已经完成并且可见；异常点之后的指令对寄存器/内存不能产生任何可见修改。

**非精确异常**
非精确异常指：异常出现时，寄存器或内存里的可见状态已经被部分“后续指令”修改过，导致状态不再对应某个清晰的程序顺序边界。此时操作系统无法保证“把异常点之前的结果保留、异常点之后的结果完全不出现”，因为后面的指令可能已经写了寄存器或内存，异常处理就失去了明确的恢复点。

---

#### 为什么 Tomasulo 机制本身不能保证精确异常

Tomasulo 的核心行为是：指令完成执行后，把结果通过总线广播并直接写回寄存器，而这些**写回发生的先后顺序取决于执行完成时间，不一定与程序顺序一致**。一旦后面的指令先完成并把结果写进了寄存器/内存，再发生异常，体系结构状态就已经包含“异常之后指令”的修改，这与精确异常要求冲突。

---

#### 乱序执行如何保证精确异常？

**乱序执行如何保证精确异常：结果先缓存在临时结构，按序提交（commit）**
要同时拥有乱序执行带来的性能和精确异常带来的可恢复性，关键是把“执行完成”与“对体系结构状态生效”分离。乱序阶段允许指令先执行完，但执行结果先进入一个临时保存的位置；只有当该指令之前的所有指令都已经被确认可以提交后，才把它的结果写入寄存器/内存。这样异常发生时，只要阻止异常点之后的指令提交，就能保证体系结构状态停在异常点之前的边界。

---

#### 重排序缓冲区（Reorder Buffer, ROB）

##### ROB定义

**ROB 是用来缓存“已执行但尚未提交”的指令结果的结构**。每条进入乱序系统的**指令会在进入保留站的同时分配一个 ROB 表项，ROB 按程序顺序分配**，因此 ROB 内天然有“程序顺序队列”的属性。ROB 的存在让硬件能区分两件事：指令是否已经算出结果（完成执行），以及指令是否已经把结果写入体系结构状态（完成提交）。

**ROB 表项包含的信息（每项内容：目标寄存器ID、指令结果、指令 Op）**
ROB 表项需要记录“提交时要写到哪里”和“要写什么”。

- 目标寄存器 ID 用于提交阶段决定写回的寄存器编号；如果是存储指令（store）或需要更新内存的指令，目标信息会对应到内存写入相关的地址/数据控制信息。
- 指令结果字段保存执行单元产生的结果值，直到提交时才真正写入寄存器/内存。
- 指令 Op 或类型信息用于提交阶段判断这条指令属于哪类更新（写寄存器、写内存、分支/异常处理等）以及提交时应当做的动作。

---

##### ROB具体做法

**寄存器重命名从“保留站ID”切换为“ROB ID”**
为了让后续指令能准确追踪“它依赖的值将由谁产生”，寄存器重命名的 tag 需要指向“最终会提交的那个位置”。**引入 ROB 后，通常把目的寄存器的重命名 tag 记为 ROB 的编号（ROB ID），而不是保留站编号**。这样做的直接效果是：后续指令如果读取到一个寄存器处于“等待状态”，它等待的是某个 ROB ID 对应的结果；当该 ROB 表项结果准备好，就可以被转发给依赖者，同时也能在提交阶段按序写回寄存器。

**写回（write-back）阶段写到 ROB，而不是直接写寄存器/内存**
**指令执行完成后，在写回阶段把结果写入 ROB 对应表项，并标记“结果已就绪”**。此时寄存器文件与内存不会被这条指令直接修改，因此即使这条指令是程序顺序中靠后的指令，它的结果也不会污染状态。后续依赖指令如果需要该结果，可以从 ROB 读取或通过结果广播获得该值，从而继续推进乱序执行。

**提交（commit）阶段：ROB 头部按程序顺序写入 register/memory**
**提交是一个额外的阶段**，其规则是：只有位于 ROB 队首（最早尚未提交的指令），并且它的执行结果已经准备好、且之前不存在需要先处理的异常/分支错误时，才允许把它的结果写入寄存器或内存。提交完成后，该指令对应的 ROB 表项被删除，队首向后推进。

**“只要指令尚未 commit，就不会修改系统状态”与异常处理的关系**
ROB 机制保证：未提交的指令即使已经算出结果，也只是停留在 ROB 内部，因此不会改变软件可见状态。发生异常时，硬件可以将 ROB 中异常指令之后的表项清空，并恢复重命名相关映射，使得异常处理看到的寄存器/内存状态严格对应到异常点之前；异常处理结束后可以选择从异常指令重新开始或终止程序。

---

##### 加入ROB实现精确异常的核心结论

**核心结论：乱序执行，但顺序提交**
乱序阶段决定“谁先执行完”；提交阶段决定“谁先对寄存器/内存生效”。通过把结果先写入 ROB、再按程序顺序提交到体系结构状态，既能利用乱序提高执行资源利用率，也能在异常出现时提供可恢复、可定位的精确边界。

---

### 分支预测

#### Last Time Predictor（只用“上一次结果”来猜下一次）

- **典型问题出现在“双层循环”这种分支行为里：**
  `for (i=0;i<100;i++)`
  `for (j=0;j<3;j++)`
  这里内层循环的分支（比如 `j<3`）在大多数时候会“跳转/成立”，但在退出内层循环的那一次会“不跳转/不成立”。如果预测器只记“上一次”，**它会在“那一次不跳转”后立刻把预测翻转，导致下一次重新进入内层循环时又错一次**，所以会出现“每次内循环导致 2 次预测错误、状态转换太快”的现象。
- 下面这类记录（N 表示预测不跳转，T 表示预测跳转）能看出“错往往集中在模式切换点”：

| 次数 | 预测(N/T) | 实际结果(N/T) | 对错    |
| ---- | --------- | ------------- | ------- |
| 1    | N         | T             | Wrong   |
| 2    | T         | T             | Correct |
| 3    | T         | T             | Correct |
| 4    | T         | N             | Wrong   |
| 5    | N         | T             | Wrong   |
| 6    | T         | T             | Correct |
| 7    | T         | T             | Correct |
| 8    | T         | N             | Wrong   |
| 9    | N         | T             | Wrong   |
| 10   | T         | T             | Correct |
| 11   | T         | T             | Correct |
| 12   | T         | N             | Wrong   |

Last Time Predictor 改进思路：加入“滞后”（不因一次例外就立刻翻转）

- 问题本质：last-time predictor 的状态变化太快（T->NT 或 NT->T 一次就翻）。
- 现实里很多分支是“mostly taken”或“mostly not taken”：意思是它绝大多数时候结果稳定，偶尔才出现一次相反结果。
- 改进做法：用 2 bits 记录历史，让预测器“需要连续多次相反结果才翻转”。这就是“滞后特性”。
- 做法细化：T 和 NT 各用两个状态表示（不是只用一个状态）。

---

#### （Key!）2 bits 饱和计数器（Two-bit Saturating Counter）

- **每个分支用一个 2-bit 计数器来做预测。**

- **2 bits 的四种状态含义（这个定义可以改，考试要看具体的题目）**是：
  `00 -> N`（强烈预测不跳转）
  `01 -> n`（较弱预测不跳转）
  `10 -> t`（较弱预测跳转）
  `11 -> T`（强烈预测跳转）
  这里大写/小写表示“强/弱”，核心是：弱状态更容易被改变，强状态不容易被一次结果改变。

- **预测时怎么用（这只是根据上面写的四个状态，考试具体考虑题目）**：

  - 状态是 `10/11` 就预测“跳转”；状态是 `00/01` 就预测“不跳转”。
  - 当真实结果出来后，计数器按规则更新：
    - 如果真实“跳转”，计数器向 `11` 方向加一格（`00→01→10→11`），但到 `11` 就不再加（这就是“饱和”）。
    - 如果真实“不跳转”，计数器向 `00` 方向减一格（`11→10→01→00`），但到 `00` 就不再减。
      这样就实现了“出现一次例外不会立刻翻转预测”，只有连续出现相反结果才会从强状态跨到另一侧。

- 结合 BTB 的结构理解“为什么需要 tag/target/2bits”：

  <img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106095144838.png" alt="image-20260106095144838" style="zoom:80%;" />

  - PC 的一部分位用于索引 BTB（找表项），另一部分位做 tag（防止不同 PC 误命中同一个表项）。
  - 表项里通常会有：tag、2-bit 预测状态、Target PC（如果预测跳转，要跳到哪里）、以及顺序执行的 `PC+4` 作为备选 next PC。这样取指阶段就能直接给出 Next PC。

- **对双层循环的效果：稳定之后“每次循环只有 1 次预测错误”。**原因是内层循环大多数次都跳转，计数器会稳定在 `10/11`，退出那次不跳转会把它从 `11` 拉到 `10`（仍然预测跳转）或从 `10` 拉到 `01`（才会翻），因此总体错的次数会减少。

- 代价与收益：预测准确率更高，但硬件开销更大；课件给出的经验数据是 2-bit 饱和计数器准确率在 85–90% 左右。

2-bits饱和计数器の状态转移图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106095328746.png" alt="image-20260106095328746" style="zoom:80%;" />

---

## 分支预测 期末题目（大题其一，Hw5）

### 题目：

假设**某处理器有一个 2-bits 的 Global History Register（GHR），由所有的分支语句共享**，其初始值为 00（表示 Not Taken）。**每个 Pattern History Table Entry（PHTE）包括一个 2-bits 的饱和计数器**，其含义如下：
00 - Strongly Not Taken
01 - Weakly Not Taken
10 - Weakly Taken
11 - Strongly Taken

假设下面的代码运行在该处理器上。该代码含有两个分支语句（B1 和 B2）。

```c
for (int i = 0; i < 1000000; i++) { /* B1 */
  /* TAKEN PATH for B1 */
  if (i % 3 == 0) {                /* B2 */
    /* TAKEN PATH for B2 */
    j[i] = k[i] - 1;
  }
}
```

(a) 有没有可能发生前 5 次循环所有分支预测全部错误？如果可能，列出每个 PHTE 可能的初始值（Not Taken 用 N 表示，Taken 用 T 表示）。

(b) 当系统达到稳定状态之后（很多次循环之后），该分支预测器的准确率是否可以达到 100%？如果可以达到，对 PHT 的初始值的设置是否有特殊要求？

---

### 解答：

这台处理器有一个 2-bit 的全局历史寄存器 GHR
GHR 里只放最近发生的两次“分支结果”，所有分支（B1、B2）都共用它
把 Taken 记为 1，把 Not Taken 记为 0
GHR 初始是 `00`，表示最近两次都是 Not Taken

它还有一张模式表 PHT（因为 GHR 是 2-bit，所以一共 4 个表项）
PHT 的每个表项是一个 2-bit 饱和计数器
`00` 强烈预测 Not Taken
`01` 轻微预测 Not Taken
`10` 轻微预测 Taken
`11` 强烈预测 Taken
预测规则很简单：`00/01` 预测 N，`10/11` 预测 T
更新规则也很简单：真实结果是 T 就“加 1”（最多到 11），真实结果是 N 就“减 1”（最多减到 00）

每次预测一个分支时，会发生什么
先看当前 GHR 是多少
用这个 GHR 去选中 PHT 的某一项（GHR=00 用 PHT[00]，GHR=01 用 PHT[01]，以此类推）
用该 PHT 项给出预测（N 或 T）
分支真正执行完，得到真实结果（N 或 T）
用真实结果更新该 PHT 项
再把真实结果这一位移入 GHR（GHR 左移一位，最低位放入本次结果）

------

接着问：前 5 次循环里，B1 和 B2 的真实结果是什么

B1 是 for 循环的分支：`i < 1000000` 时继续循环
在前 5 次循环里，`i=0,1,2,3,4` 都满足 `i < 1000000`，所以 B1 都是 Taken（记 T）

B2 是 if 分支：`i % 3 == 0` 时进入 if
i=0 满足，B2 是 T
i=1 不满足，B2 是 N
i=2 不满足，B2 是 N
i=3 满足，B2 是 T
i=4 不满足，B2 是 N

所以前 5 次循环中 B2 的真实序列是：`T N N T N`

再问：一次循环里，B1 和 B2 谁先发生
按代码结构理解：进入循环要先通过 B1（是否进入循环体），进入循环体后才会执行 B2（if 判断）
所以每次循环的分支发生顺序是：先 B1，再 B2

因此前 5 次循环的“动态分支真实结果序列”是
`B1:T, B2:T, B1:T, B2:N, B1:T, B2:N, B1:T, B2:T, B1:T, B2:N`， 即 `T T T N T N T T T N`

------

#### (a) 有没有可能让前 5 次循环里所有分支预测都错

**要让第一次预测就错，需要什么？**
第一次分支是 B1，真实是 T
当时 GHR=00，所以会用 PHT[00] 来预测
要错，**就必须让 PHT[00] 初始预测 N（也就是 PHT[00] 初始为 00 或 01**）并且在第一次预测后，PHT[00]如果一开始是01，会++变为10, 一开始是00，会++变为01（因为实际结果是T，所以得+）；

**然后再想，第二次预测错需要什么？**

第二次对应的 GHR 应该是 01（因为上一次实际是T(跳转)，所以GHR从00变为01），意思就是说PHT[01]应该还是00/01，并且在第二次预测后，PHT[01]如果一开始是01，会++变为10, 一开始是00，会++变为01（因为实际结果是T，所以得+）；

**然后第三次预测错？**

第三次对应的GHR是11（因为第一第二次的实际结果都是跳转T），这次想预测错误的话，我们还是需要预测N，那就还是对应PHT[11]一开始得是00/01然后并且还是会++分别变为01/10；

**第四次？**

第四次对应的GHR还是11，然后实际结果是N，若想错误预测为T的话，意味着现在PHT[11]就得是10/11了，那结合第三次也得预测错误，我们就推出来，**PHT[11]一开始只能是01**，这样就会在预测错第三回变为10，从而能在第四次错误的把N预测成T,然后这次错误预测后，PHT[11]就从10-- 变为01了。

**第五次？**

对应GHR为10，实际结果为T，错误预测的话，得预测为N，意味着PHT[10]得为00/01，然后预测后变为01/10；

**第六次？**

对应GHR 01，实际为N，错误的话，得预测为T，意味着现在PHT[01]得为10/11,结合第二次预测错，**可以共同推断出PHT[01]最开始为01**，原因类似PHT【11】,不再赘述，且第六次预测完后，PHT[01]会变为01（10-->01）

**第七次？**

对应 GHR 10，实际为T,错误，得预测为N，意味着现在PHT【10】得是00/01，结合第五次预测错，**可以推出PHT【10】初始得为00,**原因不再赘述，且预测完成后，PHT[10]会从01变为10；

**第八次？**

GHR=01，用 PHT[01]，真实结果 T，要错预测必须 N ⇒ PHT[01] 当下是 00 或 01，结合第六次预测结果，发现没问题，01 预测 N，真实 T，所以预测错了，满足要求，并且PHT[01]更新后变 10
**第九次？**

GHR=11，用 PHT[11]，真实 T，要错预测必须 N ⇒ PHT[11] 当下是 00 或 01，结合第四次预测结果，没问题，01 预测 N，真实 T，预测错，满足要求，PHT[11]更新后变 10。

**第十次？**

GHR=11，用 PHT[11]，真实 N（i=4），要错预测必须 T ⇒ PHT[11] 当下必须是 10 或 11，刚刚第 9 次真实 T，把 PHT[11] 更新成了 10
，10 预测 T，真实 N，所以预测错，满足要求，且PHT[11]更新后变 01

我们可以发现一个很神奇的事情，就是无法确定PHT[00]一开始究竟是00还是01，但别的都能确定，所以答案其实有两种情况：

**存在解，并且只有下面两组 2-bit 初始值能做到“前 5 次循环所有预测都错”**

PHT 初始值可行情况（GHR 作为索引）

| GHR(索引) | PHTE 初始计数器（可行） | 初始预测方向 |
| --------- | ----------------------- | ------------ |
| 00        | 00 或 01                | N            |
| 01        | 01                      | N            |
| 10        | 00                      | N            |
| 11        | 01                      | N            |

------

#### (b) 跑很久以后，这个预测器能不能达到 100% 准确率

**我们想想，跑很久之后，实际跳转的序列是啥？**

- 外层循环可以理解为不跳出去，所以一直是T。
- 内层的是每三次进去一次，出去两次，也就是 T N N。

**稳定之后，整体实际跳转序列就是？**

`T T T N T N`

**想要100%预测对，意味着预测的序列也是？**

`T T T N T N`

那我们想想有没有可能做到100%对呢：

我们可以分个类，看看对于这个序列，GHR有哪几种情况：

- GHR = 11 ---> 对应第3次预测为T,第四次预测为N;
- GHR = 10 ---> 对应第五次预测为T,第七次（下一个序列的第一个）预测为T;
- GHR = 01 ---> 对应第六次预测为N,第八次（下个序列第二个）预测为T;

看看有没有矛盾？

考虑 GHR = 11:

- 若第三次预测为T，意味着预测前其的PHT【11】为10/11，且预测后会++，变为11；
- 而想要紧接着的第四次预测为N,意味着预测完第三次后，PHT【11】得为00/01,但现在PHT【11】就是11，那是不是就是矛盾了呀？

**所以，GHR = 11 时已经矛盾！不可能为100%正确率，所以答案就是NO!**

---

那我们多思考思考，看看GHR=10/01时候矛盾否：

考虑 GHR = 10：

- 我们先看稳定后的真实动态序列 `T T T N T N`，并且把 GHR 按规则推一下：可以得到 **第 5 次预测前 GHR=10**，第 7 次预测（下个序列第一个）前也会再次出现 GHR=10。
- 那么问一句：**GHR=10 这两次对应的真实结果是什么？**
  这两次都是在预测 B1，而 B1 在稳定阶段几乎一直是 T，所以真实结果都是 T。
- 这意味着什么？
  - 若第 5 次要预测对 T，只需要 PHT[10] 在那时预测 T（计数器为 10/11）。
  - 真实结果也是 T，所以更新后 PHT[10] 只会更偏向 T（10→11 或 11→11）。
  - 到第 7 次再遇到同一个 GHR=10 时，真实结果仍然是 T，此时 PHT[10] 继续预测 T 依然能对。
- 所以结论是：**GHR=10 本身不会造成“必须一会儿预测 T、一会儿预测 N”的冲突，它反而很容易稳定成一直预测 T。**
  也就是说，**只看 GHR=10，推不出“不可能 100%”**。

---

考虑 GHR = 01：

- 我们继续按同一套推法，可以算到：**第 6 次预测（B2）前 GHR=01**，第 8 次预测（下个序列的 B2）前也会再次出现 GHR=01。
- 现在问：**这两次 GHR=01 对应的真实结果一样吗？**
  这两次都是在预测 B2。B2 的真实结果在稳定阶段是 `T N N` 循环，所以：
  - 第 6 次对应的是 B2 的 N
  - 第 8 次对应的是 B2 的 T
    也就是说：**同一个 GHR=01，第一次遇到真实是 N，紧接着下一次又遇到真实是 T。**
- 这就直接矛盾了：
  - 想让第 6 次预测对 N，意味着那时 PHT[01] 必须预测 N（00/01）。真实结果 N 后会 --，更偏向 N。
  - 但紧接着第 8 次又要求预测对 T，这意味着那时 PHT[01] 必须预测 T（10/11）。可它刚被真实 N 往 “更偏 N” 的方向更新过。
  - 反过来也一样：如果你为了第 8 次把它调到预测 T，那第 6 次就会更容易错。
- 这说明：**同一个索引 PHT[01] 在稳定运行时要同时服务两种不同真实结果（N 和 T），它不可能长期做到每次都对。**
- 所以结论是：**光看 GHR=01 也能推出不可能 100% 准确率，答案就是 NO。**

---

总结：

- **看 GHR=10：看不出矛盾，因为它反而容易一直预测 T。**
- **看 GHR=01：能直接看出矛盾，因为它会在相邻出现时对应不同真实结果（一次 N、一次 T）。**
- **看 GHR=11：也能看出矛盾（同一索引会遇到 T 和 N），所以也能否定 100%。**

---

## Chapter 5 指令并行高级技术

### 多发射（Multi-Issue）

- **多发射要解决的核心问题是：同一个时钟周期里，能不能“发射多条指令”让多个执行单元并行工作。**

#### Superscalar（超标量）

- 定义：**每周期发射多条指令，数量不固定（可以是 1 到 8 条），由编译器或硬件来调度**。
- 关键点在“动态调度”：硬件在运行时决定哪些指令能并行、怎么重排，以提高指令级并行性（ILP）。硬件需要做相关性检查、资源冲突判断、选择可发射指令等，所以硬件更复杂。
- 例子：IBM PowerPC、Sun UltraSparc、DEC Alpha、Pentium 4、i7 等。

#### VLIW（超长指令字）

- 定义：把多条指令（常见 4 到 16 条）打包成一条固定格式的长指令，由编译器静态完成调度与打包。
- 关键点在“静态调度”：硬件按编译器打包好的方式执行，因此硬件设计更简单，但编译器要做更重的分析与调度工作。
- 例子：IA-64（也称 EPIC）。

超标量 vs. 超长指令字（对比总结）

- 都能在一个周期内并行处理多条指令：超标量是“发射多条指令给多个执行单元”；VLIW 是“先把多条指令打成一个长指令，再执行”。
- 区别的决定性点是“谁来做调度”：
  - 超标量主要依赖硬件在运行时做动态调度和重排。
  - VLIW 主要依赖编译器提前做静态调度和打包。

Lys给的表格，说要记：

| Superscalar（超标量）                                        | VLIW（超长指令字）                                      |
| ------------------------------------------------------------ | ------------------------------------------------------- |
| 在一个时钟周期内发射多条指令，多个执行单元并行处理这些指令   | 将多条指令打包成一个超长指令字                          |
| 动态调度：硬件在运行时进行指令的调度和重排，提高指令级并行性 | 静态调度：编译器负责指令打包和调度，硬件设计简化        |
| 复杂的硬件设计                                               | 复杂的编译器设计                                        |
| 较高的功耗和热量                                             | 低功耗和高效性                                          |
| 灵活性高                                                     | 灵活性差                                                |
| 适合通用计算任务，如服务器、桌面计算机                       | 适合于特定应用场景，如数字信号处理器（DSP）、嵌入式系统 |
| 被广泛采用，如 Intel Core、AMD Ryzen                         | 用于一些专用处理器，如 Intel Itanium、DSP               |

---

### 多线程

#### **单核多线程执行策略**

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106115144717.png" alt="image-20260106115144717" style="zoom:80%;" />

图里纵轴是时间（每一行代表一个处理器周期往下走），横轴是一次能“发射/发出”的指令槽位数量（越往右说明同一个周期能并行发出越多条指令）。彩色方块表示该周期这些槽位被某个线程的指令占用了；白色方块表示这个槽位空着（硬件本来能发，但没指令可发或者被卡住了）。

##### **Superscalar（超标量）**

**在同一个周期里，可以从同一个线程里取出多条“互不冲突”的指令，同时发射到多个执行单元里并行跑。**
它的问题是：如果这个线程当前能并行的指令不够（比如数据相关、分支没决定、cache miss 等），那么某些槽位就会变成白色空槽，硬件能力浪费掉。

##### **Fine-Grained Multithreading（细粒度多线程）**

**每个周期换一个线程来发射指令（周期级交错）**。图里会看到：某一行的彩色方块基本来自同一个线程，但下一行换成另一个线程。
它的核心效果是：当某个线程因为“等数据/等分支结果”等原因暂时发不出指令时，处理器不需要干等，可以立刻让别的线程来占用槽位，把白色空槽填起来，提高整体利用率。

##### **Coarse-Grained Multithreading（粗粒度多线程）**

**只有当遇到“比较长的停顿事件”才切换线程（比如 cache miss 这种要等很多周期的）**。图里会看到：会连续好几行都是同一个线程在用槽位，直到出现一段明显的空槽/停顿，才切到另一个线程。
它的目标不是每周期都切，而是把“长停顿”用别的线程的执行时间盖过去。

##### **Simultaneous Multithreading, SMT（同步多线程）**

**同一个周期里发射的多条指令可以来自不同线程。**图里同一行会出现多种颜色混在一起。
这意味着：如果线程 A 只能提供 1 条可发射指令、线程 B 也只能提供 1 条，那么 SMT 可以把它们拼在同一周期的不同槽位一起发出去，进一步减少白色空槽。

---

#### **细粒度多线程：优点与缺点**

**优点**

- **能掩盖短停顿和长停顿：**当某个线程在流水线中因为数据相关、分支未决、访存等待等原因导致某些周期发不出指令时，处理器可以在这些周期让其他线程继续发射，把原本会空出来的发射槽位占起来。
- 线程切换足够频繁时，不需要像“发生重大控制流变化”那样把流水线清空再重来：因为每个周期发射时就已经在用 thread id 选好了正确的状态，流水线里本来就允许不同线程的指令并存。
- **更充分利用硬件资源**：减少白色空槽，让执行单元、流水线阶段更常处于工作状态。

**缺点**

- **单个线程完成时间会变长：**因为一个线程不能连续占用每个周期，它在时间上被分摊给其他线程了，所以“单线程延迟”增加。
- 即使某个线程本身几乎不 stall，也可能被迫让出周期给其他线程：它的进度会被整体的交错策略拖慢。
- **必须有足够多的可运行线程：**如果线程数量不够，交错之后仍然会出现白色空槽，填不满流水线/发射槽位，收益下降。

------

#### **粗粒度多线程：什么时候切换、能盖住哪些 stall**

**粗粒度的规则是：只在遇到“stall 时间很长”的事件时才切换线程**。

图里列的典型例子（可能的stall事件）：

- cache miss：需要去更下层存储拿数据，可能很多周期都等不到。
- synchronization events：例如读一个队列但队列为空，需要等别的线程/别的核心把数据放进来才能继续。
- FP operations：某些浮点运算延迟长，也可能把线程卡住较久。

粗粒度的行为特点是：一个线程可以连续使用流水线很多周期，直到它被长事件卡住才切走，因此它主要用来覆盖“长停顿”，对“短停顿”不如细粒度敏感。

------

**粗粒度多线程：优点与缺点**

**优点**

- **切换不需要那么频繁**：只有真的被长 stall 卡住才切，切换开销更少。
- **不会把“本来不怎么 stall 的线程”频繁赶下去：线程通常能连续运行一段时间，单线程体验相对更好**。
- **关键线程可以设置更高优先级：**让它在可运行时更倾向于被选中。

**缺点**

- **一旦切换，常常需要排空并重新填充流水线**：如果切换发生在流水线里已经有很多该线程的在途指令，为了保证状态一致，可能要让这些在途指令完成或被清掉，再把新线程的指令灌进来；流水线越深，这个代价越明显。
- **公平性问题：stall 少的线程可能长期占用流水线，stall 多的线程可能长期得不到运行机会**；常见做法是给**每个线程配额**，超过一定占用时间就强制切换。

------

#### **同步多线程 SMT：它比前两种多做了什么**

**SMT 的要点是：同一个周期发射的多条指令可以来自不同线程。**
这样做的直接结果是：

- 保持多个执行单元的高利用率：如果某个线程在当前周期只能拿出少量可发射指令，SMT 可以用其他线程的可发射指令把剩下的槽位补齐。
- 更好的流水线效率：减少同周期内的空槽。
- 多线程在流水线里是“同时存在”的：不需要频繁做“把一个线程换出去再换回来”的上下文切换动作，因为不同线程的指令本来就在并行推进。

------

**SMT 的硬件资源**

| 资源类别                                               | 图中列的资源                                                 | 这些资源为什么要这样处理                                     |
| ------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **需要复制的资源（每个线程都有一份）**                 | return address stack；**PC**；**architected register file**；control logic/state | 这些是“架构上必须独立”的线程状态。PC 决定每个线程下一条从哪取指；**architected register file 是程序员可见的寄存器集合，每个线程必须各自保存自己的寄存器值；**return address stack 用来预测/恢复函数返回地址，属于控制流相关的线程私有信息；control logic/state 指和该线程相关的控制状态（例如该线程是否可运行、异常/中断相关标记等），不能和别的线程混用，否则线程状态会互相覆盖。 |
| **不需要复制，但需要划分的资源（每个线程分到一部分）** | reorder buffer；load/store buffers；various queues（例如 scheduling queue） | 这些属于“动态执行/乱序/调度过程中用的容量型结构”。ROB 记录还没提交的指令结果，必须区分属于哪个线程，但没必要给每个线程做一整套独立硬件；更常见是一个总的 ROB/队列，里面的 entry 标记线程归属，并对每个线程限制可用的 entry 数量，避免某个线程把结构占满导致其他线程无法发射/提交。load/store buffers 和调度队列同理：它们是共享的“等资源”，但需要按线程做配额与隔离。 |
| **多线程共享的资源**                                   | **Caches（L1/L2/L3）；microarchitecture registers**（例如 physical registers）；execution unit | 这些是“算力与存储层级”的公共硬件。缓存层一般服务所有线程；执行单元（ALU、FP、Load/Store 单元等）本来就可以在不同周期/同周期处理来自不同线程的指令；microarchitecture registers 这里指的是微结构层面的资源（例如**重命名后的物理寄存器池），通常作为一个共享池来提高利用率**，但同样需要用分配策略保证不同线程都能拿到资源、并在回收时不混乱。 |

---

## Chapter 6 多处理器分类

### 多处理器分类（略）

松耦合多处理器/紧耦合多处理器 略

---

### Cache一致性

**定义：**

如果多个处理器同时拥有同一个 cache block（也就是同一段内存数据在多个 cache 里各有一份），就会出现一个核心问题：大家手里拿的“这份数据”会不会不一样。只要有一个处理器在自己本地把这份数据改了，其他处理器的 cache 里可能还是旧值，这就叫“不一致”。这个问题在 write-through 和 write-back 两类 cache 里都会出现。

**维护 Cache 一致性（Coherence）：**
**一致性的定义很直白：对同一个内存位置，所有处理器最终看到的值要一致**，不能你读到 5、我读到 7。为了做到这一点，核心思路是：**写操作发生时，要让“最新值”及时影响到系统里其他 cache 的副本。**

---

**写操作常见两种做法**

- **更新式（update protocol）**：谁写了新数据，就把新数据广播给所有持有副本的 cache，让大家都把自己的副本改成最新值。
- **使无效式（invalidate protocol）【key！】**：谁要写，就先让其他处理器手里的副本全部变成“无效”，只保留自己这一份是有效的，然后自己更新。这样系统里同一时刻只会有一份有效副本能被继续使用。

---

**消息如何传播**
要把“更新”或“使无效”的信息送到其他处理器，有两种典型组织方式。

**侦听式（Snoopy bus）【Key！】**
**处理器把自己的读/写相关动作在总线上广播**；其他处理器的 cache 会**一直“监听”总线上的这些动作**，看到相关消息就更新自己的状态。所有请求通过同一条总线会被顺序化，所以大家看到的先后顺序是一致的。

**目录式（Directory）**
系统用一个“目录”去记录每个 block 目前在哪些 cache 里有副本、谁是所有者。处理器要数据时通过目录显式请求；目录负责协调要不要发 invalidate、要不要发 update，并且在出现乱序请求时起到“把顺序理清楚”的作用。

读这页时要抓住一句话：侦听式靠“总线广播+大家监听”，目录式靠“集中记录+集中协调”。

---

**总线侦听的作用（侦听式为什么能工作）**
当**某处理器写了本地副本，它会把这次写相关的信息广播出去，其他 cache 也能看到同样的更新影响**，这叫 write propagation，意思是“写的影响能传播出去”。

当两个处理器几乎同时写同一份数据，**总线一次只能让一个人先用，**于是会出现“一个赢得总线、另一个等待”。结果是：所有其他 cache 看到的写入顺序是同一个顺序，这叫 write serialization，意思是“写的先后顺序被统一了”。

这两点合起来保证：**大家最终不会各自形成不同的写入顺序**。

---

**更新式 vs 使无效式：代价对比**
**更新式（update-based）**

优点：如果**数据更新不频繁，就可以避免使无效带来的额外成本**，因为使无效会让别人下次读的时候必须重新加载（re-load）。

缺点：如果**写得很频繁**，而且别的处理器还没来得及读这份数据就又被重写了，那么你广播出去的更新可能完全没人用到，等于**白发了很多消息**。

**使无效式（invalidate-based）**

优点：**广播使无效后，写入者会拥有唯一有效副本**；只有真的又来读取的处理器才会重新获得本地副本。

缺点：**被你“使无效”的处理器下次要读就会变慢**，因为必须 re-load；如果很多处理器在激烈竞争写同一份数据，会出现 **ping-pong 效应，也就是“使无效 → 读回来 → 又被使无效”反复发生**，总线消息和等待都会很多。

---

#### **Write-back Cache：MSI 协议**

**write-back 的特点是：写的时候不一定立刻写回内存，可能先留在 cache 里，等替换出去才写回**。所以系统更需要用“状态”来说明：某份数据到底谁手里是最新的、内存是不是最新的。

MSI 把每个 cache block 的状态分成三类

- **Invalid（I）：本地没有这份数据副本**。
- **Shared（S）：本地有一份只读副本，而且这份数据是最新的；同时别人也可能有同一份只读副本**。
- **Modified（M）：本地有全局唯一的有效副本，别人都没有；这份副本可能和内存不一致（dirty），本地允许写，而且写的时候不需要通知别人，因为别人没有副本可用。**

处理器可能做的动作
load、store、Evict（这块 cache 被替换出去）。

总线事务（在总线上发的请求）
BusRd（为 load 服务）、BusRdX（为 store 服务，含“我要独占来写”的意思）、BusWB（write back，把脏数据写回）。

---

##### MSI 状态转换

表格：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106150027441.png" alt="image-20260106150027441" style="zoom:80%;" />

- 现在是 I，如果本地要 load（读数据）：这是 load miss，要发 BusRd，把数据拿回来，状态变成 S（因为读回来的数据可能也被别人读过，所以先当作共享只读）并且如果别的cache在S，就无影响；但若某个cache在M，cache状态就会从M退化到S并且将数据WB写回内存 。
- 现在是 I，如果本地要 store（写数据）：这是 store miss，要发 BusRdX，拿到独占写权限后状态变成 M；若别的cache为S，就会被无效化，由S->I，若为M，则会从M->I并且触发WB。
- 现在是 S，如果本地要 store：你手里只是只读共享副本，想写必须升级权限，所以会触发总线动作（典型就是 BusRdX），把别人无效掉（把S状态的都变为I），然后自己进入 M。
- 现在是 M，别人来读（对方 load miss）：你手里的数据可能是最新但还没写回内存，所以通常需要把最新值提供出去，并且你的状态会从 M 降到 S（因为别人也拿到了一份副本，你不再是唯一）。
- 现在是 S 或 M，别人来写（对方 store miss）：为了让对方独占写，自己的副本要变成 I（无效）。

重点： 在 MSI 协议之中，是**不会有 M 和 S 共存的情况**的，只可能 **1个M + 若干个I / 若干个S + 若干个 I** 的情况。

---

##### **MSI 协议存在的问题**

如果本地其实是“全局唯一的一份副本”，但它却处在 S 状态，那么本地一旦要写，就会广播 BusRdX 让自己升级成 M，再写。可问题是：既然全局只有这一份副本，本来就不需要广播去让别人无效，因为根本没人持有副本。这样会产生不必要的总线消息。

---

#### MESI（对MSI的改进）

解决方案：MESI
思路是**增加一个 Exclusive（E）状态，用来表达“全局唯一 + 干净（clean，和内存一致）”。这样从 E 写到 M 时可以直接升级，不需要广播消息。**

怎么进入 E 状态

- 读一个 block 的时候，如果其他 cache 都没有这份数据，那么你读到后就可以直接标成 E。
- 系统一开始可能有多个 S 副本，但因为 cache 替换，最后只剩一个 S 副本时，这个剩下的副本也应该变成 E（因为它已经变成全局唯一且是干净的）。

**E 的意义就是“我一个人拿着最新干净副本，所以我写的时候可以少一次广播”。**

---

**侦听式 Cache Coherence（实现角度）**
如果所有 cache 共享一条总线，实现一致性会相对直接：每个 cache 在总线上广播自己的 read/write 相关操作；每个 cache block 用一个有限状态机（FSM）管理自己的状态，逻辑会比较规整。

换而言之，我们下面讨论的MESI就是基于侦听式（Snoopy bus）来实现的，记住这个就OK：

**MESI Protocol**

- Modified（M）：只有我有，且我改过，内存可能不是最新。
- **Exclusive（E）：只有我有，但我没改过，内存也是最新。**
- Shared（S）：不止我一个人有，大家都是只读副本，数据是最新的。
- Invalid（I）：我这里没有这份数据。

---

##### MESI 状态转换

状态转换图：

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106151528464.png" alt="image-20260106151528464" style="zoom:80%;" />

---

**MESI 状态转换里，最该记住的变化规律**

- **I 读（load miss）会变成 S 或 E：**如果别人也有这份数据，一般进入 S；如果别人都没有，一般进入 E。
- E 本地写可以直接到 M：这是 MESI 相比 MSI 最关键的优化点，避免了“明明没人共享还要广播”。
- S 本地写通常需要先让别人无效，自己到 M：因为共享状态下写必须先拿到独占。
- M 遇到别人读，会把别人带进来，于是自己不再唯一，往 S 方向变化；M 遇到别人写，自己会被无效到 I。
- **KEY：若某个cache状态为E且其本地store，它会自己默默升级为M，不广播！！！！！！！！**

---

##### MESI 协议存在的问题

**问题：**

- **Shared（S）状态的要求**：处于 S 的这条 cache line 必须是 **clean（干净的）**，意思是“数据是最新的，而且内存里的那份也是最新的”。换句话说，如果某个 block 现在是 S，那么任何一个共享者（包括内存）拿到的数据都不能是旧的。
- **问题从哪里来**：如果某个 block 在某个核的 cache 里是 **M（Modified）**，就表示这条数据被这个核改过了，**cache 里的数据是最新的，但内存里的数据是旧的（dirty 相对内存）**。
- **冲突点**：当“别的核要读这个 block”时，系统希望让它进入“共享（S）”，但 **S 又要求内存必须最新**。所以在纯 MESI 的语义下，经常会出现一种被迫操作：
  - 先把 M 里的最新数据 **写回内存（write back）**，把内存更新成最新
  - 然后大家才能都处于 S（因为这时内存也是最新的，满足 clean）

- **为什么这是个问题**
  - **开销不必要**：如果每次从 **M → S**（别人读导致你从独占改写变成共享）都强制把数据写回内存，那么会产生大量内存写流量。
  - **很多写回很快就“白写”**：因为可能没过多久，某个核又对同一块数据写了一次，内存又立刻过期。也就是说，“刚写回内存的那次更新”很可能马上又被新的写覆盖意义。

---

**MESI 的改进思路**

- **核心想法**：当发生 **M →（别人读）** 的时候，不一定要立刻把最新数据写回内存。
- 具体做法可以概括成两句
  - 发生 M → S 这种“别人来读”的情况时：**只把最新数据给请求者**（让请求者读到对的值），但**不立即写回内存**
  - 这条 cache line 真的要被替换（evict）出 cache 时：再按 **write-back** 的原则把它写回内存

- **新挑战：如果不立刻写回内存，会出现什么麻烦**
  - 如果你不写回内存，但又让多个 cache 都拿到了这份数据并处于“共享”关系，那么就会出现一个关键问题：
    - **内存现在还是旧的**
    - **多个 cache 里可能都有“最新数据”**
    - 那到底 **谁负责将来把最新数据写回内存**？

---

#### MOESI （对MESI的改进）【说是不考，知道MESI还有啥问题就行】

**MOESI 怎么解决 MESI 的这个问题**

- **MOESI 增加了一个新状态：O（Owner）**
- 发生 **M →（别人读）** 时，不再强行变成“大家都是 S + 内存最新”，而是变成：
  - 共享者里 **选一个** 持有者被标为 **Owner（O）**
  - **O 这份一定是 dirty 的**（它保存着“相对内存更新的那份最新数据”）
  - 其他拿到数据的共享者可以是 **S（它们拿到的数据内容是最新的，但它们不负责写回内存）**
  - **内存允许暂时保持旧值**，因为系统明确知道“谁手里握着最新值”：就是 O
- **谁负责写回？**
  - 只要这条 line 还存在，**Owner（O）负责在它被替换出 cache 时写回内存**
  - 这样就避免了“多个 shared copy 都去写回一次”的浪费
- 用一句话总结：
  - **MESI 的麻烦是：想共享就要求内存也必须最新，因此 M 被别人读时常被迫立刻写回内存**
  - **MOESI 的解决是：允许内存暂时不最新，但用 O 明确指定一个‘持有最新数据且负责最终写回’的缓存副本**

---

### 内存一致性

**内存一致性模型可以当成“硬件和软件之间的约定”，意思是软件写并行程序时可以对“内存读写看起来像什么样”做哪些假设，而硬件的内存系统必须对外表现出符合这些假设的行为**，这样程序才不会出现“看起来很奇怪”的结果。

**内存一致性 VS 缓存一致性：**

缓存一致性（coherence）更像是在管“同一个内存地址/同一个变量”的事：如果很多处理器都在读写同一个位置，那么大家看到的这个位置上的写入顺序必须一致，也就是所有处理器对“这个单独位置”的写入先后顺序要达成一致，否则同一个变量会在不同核上出现互相矛盾的历史

内存一致性（consistency）管得更大，它在管“不同内存地址之间的先后关系”：比如一个核先写 X 再写 Y，另一个核读 Y 之后读 X，这两个地址的读写在不同核上能不能被看成同一个全局先后顺序、读能读到什么值，这些都由一致性模型来规定；所以可以把它理解成“跨多个变量的读写顺序规则”，它决定了并行程序里哪些重排是允许的、哪些结果是合法的

------

#### 顺序一致性（sequential consistency, SC）

顺序一致性的核心意思是：整个并行程序的执行结果，必须等价于“把所有处理器的内存操作按某一种全局顺序交错排成一条线来执行”的结果，同时对每个处理器来说，它自己发出的内存操作在这条全局顺序里不能乱，必须保持和程序里写的顺序一致；**换句话说，每个核内部的顺序不能被打乱，只能在不同核之间穿插**

为了更好理解“交错但不乱每个核内部顺序”，看两个具体程序和它们可能的执行顺序

处理器 P1：先写 A（A++），再读 B
处理器 P2：先写 B（B++），再读 A
初始 A=0，B=0

用“全局交错顺序”来想，下面这些交错都符合 SC，因为每个处理器内部都还是“写在前、读在后”

- 交错方式是 P1 写 A → P1 读 B → P2 写 B → P2 读 A，这时 P1 读到的 B 还是 0，而 P2 读到的 A 已经是 1，所以输出可能是 (B=0, A=1)
- 交错方式是 P2 写 B → P2 读 A → P1 写 A → P1 读 B，这时输出可能是 (B=1, A=0)
- 交错方式是 P1 写 A → P2 写 B → P1 读 B → P2 读 A，这时两边都可能读到 1，所以输出可能是 (B=1, A=1)

---

再看一个更“跨变量顺序”的例子

处理器 P1：先写 X=1，再写 Y=1
处理器 P2：先读 r1=Y，再读 r2=X
初始 X=0，Y=0

在 SC 下，如果 P2 读到 r1=1（说明它读 Y 的时候已经看到了 P1 写 Y=1），那 P2 接下来读 X 时就不应该再读到 0，因为在 P1 内部“写 X”本来就在“写 Y”之前，既然你都看到了后面的写 Y，那前面的写 X 也必须已经在那条全局交错顺序里发生过了，所以 r1=1 且 r2=0 这种结果在 SC 下是不允许的；允许的情况会是这些交错之一

- P2 两次读都在 P1 写之前发生：P2 读 Y → P2 读 X → P1 写 X → P1 写 Y，于是 (r1=0, r2=0)
- P1 两次写都在 P2 读之前发生：P1 写 X → P1 写 Y → P2 读 Y → P2 读 X，于是 (r1=1, r2=1)

---

**PPT例子：**

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106161436524.png" alt="image-20260106161436524" style="zoom:50%;" />



---

**顺序一致性的主要问题是:**

**它在硬件上很难高效实现**，因为要想“看起来像一条全局顺序”，最直接的实现会逼着硬件做很多保守限制，比如不让多个内存访问真正并发、每个节点都严格按顺序访问内存、甚至从根上就把乱序执行这类优化空间压得很小，这样性能很难上去

**顺序一致性还有“不必要的限制”这一面：很多并行程序的某些内存操作其实可以安全重排（重排也不会破坏程序逻辑），但 SC 不允许**你这么做，硬件和编译器就少了大量可以用来提速的优化手段

解决思路通常是“把规则放宽一点”，让程序员在关键地方多给一些明确提示（比如哪里必须保持顺序、哪里允许更松），让硬件/编译器在不破坏这些关键提示的前提下大胆优化，所以会引出“削弱或放宽内存一致性模型”的一系列更弱的模型来配合使用。

---

#### 【不考】弱一点的模型（Weak Consistency）

程序员在程序中插入 **fence 指令**（fence 可以理解成“顺序必须在这里卡一下”，目的是让前后的内存读写在别的处理器看来也按要求先后发生）

- **fence 之前的指令一定比 fence 之后的指令早完成**（这里的“完成”不只是本核做完，还包含“该生效的效果已经对外可见”，否则别的核可能还看不到）
- **所有在 fence 之前的数据操作一定比 fence 早完成**（数据操作主要指对内存的读/写，比如 load/store；意思是 fence 之前那些读写不能拖到 fence 之后才对外生效）
- **所有在 fence 之后的数据操作一定比 fence 晚完成**（fence 后面的读写不能抢跑到 fence 前面就“先被别的核看到”，否则程序以为“先写 A 再写 B”，别人可能先看到 B）
- **所有的 fence 按照指令顺序完成**（如果程序里有多个 fence，它们之间也不能乱序完成，否则同步点本身就不可信）

同步原语，例如 **barrier**，可以用作 fence（barrier 是一种“大家在这里对齐”的同步点；把它当 fence 用，就是让在这个点之前该传播出去的读写先传播出去、该等待的先等待，保证跨线程/跨核看到的顺序满足要求，课件里说它“在特定的点上传播读写操作”，就是在同步点把该可见的更新变成可见）

------

**优缺点：**

- **Advantage**
  - **不用保证严格的内存操作顺序**（也就是硬件可以为了快，把某些读写在不影响“同步点语义”的前提下提前/推后；比如把写先放进写缓冲、把后面的读先做等，只要不跨过需要的 fence）
  - **让硬件有更多的性能优化空间**（限制少了，CPU 和缓存系统就更容易做乱序、合并写、隐藏延迟，从而更快）
- **Disadvantage**
  - **程序员负担加重（need to get the “fences” correct）**（也就是你必须在“该保证别人一定看见某些写入之后才能继续”的地方放对 fence/barrier；放少了会错，放多了会慢）

------

#### 【不考】更弱一点的模型（Release Consistency）

更弱的模型：**Release Consistency**（它把“一个大的 fence”拆成两类更精准的同步点，让约束只围绕同步操作生效，非同步部分更自由）

- **将 fence 分成两个：一个保证在此之前完成，一个保证在此之后完成**（拆开之后，你可以只约束“进入临界区之前”或“离开临界区之前”的那一部分内存读写）
- **Acquire**：在 **acquire 之后** 的访存指令必须在 **acquire 完成之后** 完成（访存指令就是读/写内存；意思是你做了 acquire 之后，后面的读写不能跑到 acquire 之前去“先发生/先对外可见”，这样才能保证“拿到锁/进入临界区之后再读到的数据”是靠谱的）
- **Release**：在 **release 之前** 的访存指令必须在 **release 完成之前** 完成（意思是你执行 release 之前，临界区里该写出去的内容必须先写出去、先对外可见；否则你都 release 了，别人 acquire 进来还看不到你刚才写的更新，就会出错）

However（这里是在强调它“更弱”的地方，也就是只在同步点附近管顺序，其他地方不管）

- **Acquire 之前的指令完成时间无限制**（也就是 acquire 之前那堆读写，硬件怎么提前/推后都行，只要不违反上面“acquire 之后不能跑到 acquire 之前”的规则）
- **Release 之后的指令完成时间无限制**（release 之后那堆读写也可以自由优化，只要不把 release 之前该完成的东西拖过 release）

---

PDF page 710 - 857 略

---

## Chapter 7 GPU

### 【key】GPU设计思路

#### 思路1：简化流水线，把省下的空间用来增加核心数

在很多 CPU 里，为了让单线程跑得快，会加入很复杂的东西，比如乱序执行、分支预测、复杂的调度与重排逻辑，这些东西本质上是在“帮同一个线程跑得更快”。
**GPU 的做法是反过来：把这些复杂逻辑尽量去掉，让流水线变得更简单，**这样晶体管用得少、功耗更低、芯片面积也省出来了。
**省出来的资源不浪费，而是拿去增加更多运算核心（更多 ALU 之类的**执行单元），让很多数据能并行算，从而把总吞吐量做上去。

------

#### **思路2：单指令多线程（SIMT），让“管指令”的成本被很多运算单元分摊**

**SIMT 的核心意思是：很多线程在同一个时刻执行“同一条指令”，但每个线程用的是自己的数据、自己的寄存器状态。**
这样做为什么省？因为“取指、译码、发射指令”这些事情，本来每个核心都要做一遍；现在多个线程/多个执行单元一起用同一条指令流，这部分开销就被摊薄了，更多硬件资源用在执行上。

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106163401809.png" alt="image-20260106163401809" style="zoom:50%;" />

图里常见结构是：

- 前端（取指/译码）相对共享：只需要把一条指令取出来、译码一次。
- 后端（执行）有很多份：很多 ALU/执行通道并排工作。
- 每个线程有自己的“执行上下文（execution context）”：主要就是寄存器等状态，各线程互不混用。

------

**SIMT 里的常用词（NVIDIA 常用叫法）**

- **thread（线程）**：一个独立的执行单位，有自己的寄存器状态。
- **warp（线程束）**：一组被绑在一起、同一时刻执行同一条指令的线程集合。可以把它理解成“硬件一次一起发射的一组线程”。
- **SM（Streaming Multiprocessor）**：NVIDIA 的叫法，指 GPU 里的一个执行模块（里面有很多执行单元、寄存器、调度器等）。一个 GPU 会有很多个 SM。

这里要抓住一个最关键点：
同一个 warp 里的线程，在某一拍发射出去的指令是同一条；但每个线程用的数据不同、寄存器不同，所以能同时算很多份数据。

------

#### **思路3：同时驻留大量线程，让同一个核心里同时驻留“远多于执行单元数量”的线程**

为什么要在一个核心里放很多线程的上下文（寄存器等状态），甚至数量远超执行单元？原因是访存等操作会很慢，执行单元经常会等数据。
**GPU 的做法是：**

- **一个线程遇到慢操作（比如读内存）时，不让执行单元空着；**
- **立刻切去执行另一个已经就绪的线程。**

这里“切线程”之所以能很快，是因为线程的寄存器状态在硬件里已经准备好（驻留着），不需要像操作系统那种复杂切换。<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106163548387.png" alt="image-20260106163548387" style="zoom:50%;" />

图里常用的表达就是：

- 执行单元（比如 8 个 ALU）数量固定；
- 线程上下文（比如 18 份寄存器上下文）明显更多；
  表示“一个核心里养着很多线程，随时挑能跑的来发射”，用这种细粒度调度来掩盖高延迟。

------

**这一段内容要你最终记住的结论**

- GPU 不是靠把一个核心做得很聪明来提速，而是靠很多执行单元一起算来提速。
- 为了放更多执行单元，GPU 会把很多复杂控制逻辑简化掉。
- SIMT 的关键是：同一条指令让很多线程一起做，但每个线程有自己的一份寄存器和数据。
- 因为内存等待很慢，GPU 会在一个核心里同时驻留很多线程，谁能跑就切谁来跑，让执行单元尽量别闲着。

---

### GPU线程调度？

暂略，估计不考，复习PPT Page 126 - 132

---

### GPU控制流问题

- GPU 会用 **SIMD（同一条指令同时控制多条“执行通道”）** 来节省控制逻辑的面积【设计思路2】
  这句话想表达的是：GPU 不想给“每个线程”都做一套完整的取指、译码、分支判断那套控制电路，所以它把很多线程绑成一组来控制。
  这组线程就叫 **warp（线程组）**。

- 于是出现一个自然问题：**同一个 warp 里的线程，必须同一时刻走同一条指令吗？**
  正常情况下是的：warp 里线程共享一个“当前执行位置”（也就是共享一个 PC 的效果），所以它们会一起执行同一段代码。

- 那再问：**如果遇到 if/else 这种分支，warp 里有些线程条件为真、有些为假，会发生什么？**

  <img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106164623991.png" alt="image-20260106164623991" style="zoom:50%;" />

  

- 这就是图里写的 **分支分歧（branch divergence）**：
  warp 内线程会“分到不同执行路径”，比如图中分成 **Path A** 和 **Path B**。

- 再问关键点：**分支分歧为什么会让 GPU 难受？**
  因为 warp 只有一套控制流，它没法让 Path A 和 Path B 同时并行跑。它只能：

  - 先执行 Path A（此时只有走 Path A 的线程在干活，走 Path B 的线程被暂时关掉）
  - 再执行 Path B（反过来）
    图右边蓝色箭头的密度变化，就是在表达：有一段时间只有部分线程在做事，另一部分线程空着，这就是分歧带来的效率损失。

- 图里的结论句是在提醒你：
  “本来 warps 内的线程执行相同的指令，分支分歧后线程执行的指令不一样”
  说白了就是：**同一组线程本来要整齐划一，一旦 if 条件不同，就必须拆开轮流执行两条路。**

------

**GPU 是怎么“处理分支分歧”的**

- 先问：**GPU 处理分支分歧时，最需要记住什么信息？**
  至少要记住两类东西：
  - **哪些线程现在要执行**（哪些暂时不执行）→ 用一个位向量表示，叫 **掩码 mask / active mask**
  - **下一段该去哪里执行** → 用 **PC（程序计数器）** 指向代码位置
- 图里给出的做法是：**每个 warp 用一个 stack（栈）来存“不跳转分支的 PC 和掩码”**
  这句话你可以理解成：
  **一旦发生分歧，GPU 会把“另一条路”先记在小本本里（栈里），等当前这条路执行完，再翻出来执行另一条路。**
- 再问：**具体遇到分支时，栈里压什么、当前执行又改成什么？**
  图里写的是下面这些动作，把每句翻成大白话就是：
  - 遇到分支时：**将当前掩码入栈**
    意思是：先把“分歧之前，哪些线程原本是活跃的”保存起来，后面还要用来恢复。
  - 遇到分支时：**将不跳转分支的掩码和 PC 入栈**
    意思是：**把“没被你现在选中去执行的那条路”也记下来**：
    - 那条路从哪里开始执行（PC）
    - 哪些线程属于那条路（mask）
  - 遇到分支时：**设置当前掩码为跳转分支的掩码**
    意思是：**现在先跑“跳转那条路”，所以只让属于这条路的线程保持 active，其余线程临时 inactive。**
  - 分支执行完时：**将不跳转分支的掩码和 PC 出栈，并执行分支**
    意思是：**当前这条路跑完了，就把刚才记下的“另一条路”拿出来，切过去执行。**
  - 不跳转分支执行完时：**将分支之前的源掩码弹出**
    意思是：**两条路都跑完了，回到分歧发生之前的状态，把所有原本该活跃的线程集合恢复回来（准备去“汇合点”之后继续）**。
  - **如果一个分支的掩码都是 0**：**跳过该代码块**
    意思是：**如果这条路径上根本没有任何线程需要执行，那就别浪费周期了，直接不跑。**

------

**图里的“分支分歧解决方案”到底在表达什么**

<img src="C:\Users\Matthew\AppData\Roaming\Typora\typora-user-images\image-20260106164912187.png" alt="image-20260106164912187" style="zoom:50%;" />

图左边是控制流图（A、B、C、D、E、F、G 这些代码块），每个块后面像 `A/1111` 这种标记，含义是：

- `A/1111`：执行到代码块 A 时，4 个线程都活跃（1111 表示 4 个线程全开）
- `C/1001`：执行到代码块 C 时，只有线程 1 和线程 4 活跃（1001）
- `D/0110`：执行到代码块 D 时，只有线程 2 和线程 3 活跃（0110）
- `E/1111`：在 E 这个点又汇合了，4 个线程重新一起走

图右下角写着 **Thread Warp / Common PC**，想表达两点：

- warp 里有多个线程（这里画了 Thread 1~4）
- 它们共享一个“当前执行位置”（Common PC 的效果），所以必须通过 **active mask** 来决定“此刻哪些线程真的在执行”

图右上角给了一个 **Stack** 表格（你可以按“栈顶在表格下面”或“栈顶在表格上面”去理解都行，重点是字段含义）：

| Reconverge PC（汇合点） | Next PC（接下来要去跑的路径起点） | Active Mask（哪些线程跑这条路） |
| ----------------------- | --------------------------------- | ------------------------------- |
| -                       | E                                 | 1111                            |
| E                       | D                                 | 0110                            |
| E                       | C                                 | 1001                            |

每一列的大白话含义：

- Reconverge PC：这次分歧最终要回到哪里汇合（图里是 E）
- Next PC：当前这次要切去执行的那条路径，从哪里开始（比如 C 或 D）
- Active Mask：切去这条路径时，哪些线程要开工（比如 1001 或 0110）

再对照图下方的时间轴（Time）和红框执行序列：

- 时间轴上标了 `A B C D E G A ...`
  这不是说代码里就只剩一条直线，而是在说明：发生分歧后，warp 会把不同路径 **拆开按顺序执行**，于是宏观上看起来像“先跑完 C 再跑 D（或者反过来），最后在 E 汇合”。
- 每个红框里蓝色箭头的“多少”，对应 active mask：
  - 箭头多：活跃线程多
  - 箭头少：活跃线程少（其余线程在这一段时间就是 idle）

所以这张图的核心信息就是一句话：
**GPU 用“掩码 + 栈”把分歧路径拆成多段顺序执行，并在指定的汇合点 reconverge，把线程重新合并继续跑。**

总结：

**同一 warp 遇到 if 分歧时，会把两条路“按顺序”执行；两条路都执行完，会在一个汇合点恢复成同一批活跃线程，然后继续往下。**

更具体点？

**同一 warp 分歧时，GPU 用掩码把线程分成两批，先跑一批再跑另一批，期间共享同一个 PC；两批都跑完，在 reconvergence 点把掩码恢复成原来那批线程，再继续执行。**

---

## GPU 期末题目（大题其一，Hw8）

### 题目：

GPU 利用率通常定义为“处于 busy 状态的 GPU 核心（PE，计算单元）占所有 GPU 核心的比例”。也就是某一拍（或某段执行期间），有多少个核心真的在干活，占总核心数的多少。

考虑下面代码片段。
A、B、C 三个数组的数据已经在寄存器中（所以这里不需要考虑从内存读数据的等待）。
每个 thread 只执行循环中的一次迭代（题目括号里提示“包含 6 条指令”，意思是最完整路径会走到 6 条）。
一个 warp 有 64 个 threads，这个 GPU 一共有 64 个核心。
数组 B 的每个元素绝对值都小于 10。

```c
for (i = 0; i < 1024; i++) {
    A[i] = B[i] * B[i];
    if (A[i] > 0) {
        C[i] = A[i] * B[i];
        if (C[i] < 0) {
            A[i] = A[i] + 1;
        }
        A[i] = A[i] - 2;
    }
}
```

**Q1:执行该代码段需要多少个 warps**
**Q2:执行整个代码段，最大的 GPU 利用率可能是多少**
**Q3:获得最大的 GPU 利用率时，数组 B 的值有什么特征**
**Q4:执行整个代码段，最小的 GPU 利用率可能是多少**

---

### 解答

**先把必用名词讲清楚：**

- thread 是一个执行“同一段代码”的小执行单元，thread 多了就能并行处理多个 i。
- warp 是 GPU 调度的基本组：一次发射（issue）一条指令给一个 warp 的 64 个 threads 一起做。
- PE / GPU 核心：可以理解为“执行指令的计算槽位”。题目说 GPU 有 64 个核心，刚好等于一个 warp 的 64 个 thread，所以“一个 warp 全员都在干活”时理论上能把 64 个核心占满。
- busy：这个核心这一拍在执行有效工作（不是空转）。
- 控制流分歧：同一个 warp 里，有的 thread 进入 if，有的不进入；GPU 仍然按“同一条指令流”推进，只是用 active mask 把不该执行的 thread 暂时关掉，于是会出现“发射了指令，但只有部分核心真的干活”。

---

**这段代码里，分歧可能出现在哪里？**

- 外层 if (A[i] > 0) 会让一部分 thread 进块，一部分不进块。
- 内层 if (C[i] < 0) 会让已经进外层块的 thread 再分成两部分：做 A[i]=A[i]+1 或不做。

---

**问题1：执行该代码段需要多少个 warps？**

一共需要多少个 threads 来覆盖 1024 次迭代?
**题目明确说“每个 thread 执行循环中的一次迭代”，那 1024 次迭代就需要 1024 个 threads。**

再问：一个 warp 装多少个 threads
题目中说了：一个 warp = 64 个 threads。

那需要多少个 warps 才能装下 1024 个 threads
列式子计算：`1024 ÷ 64 = 16`
**Answer: 所以需要 16 个 warps。**

---

**问题2：执行整个代码段，最大的 GPU 利用率可能是多少?**

先问：什么时候 GPU 利用率最高
当一次发射的指令里，warp 的 64 个 threads 都是 active（都需要做这条指令），那 64 个核心都 busy，利用率就是 64/64 = 100%。

再问：这段代码能不能做到“每条会被执行的指令，都让 64 个 threads 一起执行”
**关键在于：能不能让一个 warp 内的 threads 对两个 if 的判断结果完全一致，这样就没有分歧。**

看外层 if：`A[i] = B[i]*B[i]`
A[i] 一定是 ≥ 0。什么时候 A[i] > 0 成立
当且仅当 B[i] ≠ 0（因为只有 B[i]=0 才会平方后等于 0）。

看内层 if：`C[i] = A[i]*B[i] = (B[i]^2)*B[i] = B[i]^3`
C[i] < 0 什么时候成立
当且仅当 B[i] < 0（并且 B[i] 不能是 0，因为 0 已经不会进入外层 if）。

**所以如果我们安排 B 的值，让同一个 warp 里的 64 个 B[i] 在符号/是否为 0 这件事上完全一致，就不会分歧。**比如：
这个 warp 的 64 个 B[i] 全都 > 0（全进外层 if，但都不进内层 if）
或者全都 < 0（全进外层 if，也全进内层 if）
或者全都 = 0（全都不进外层 if）

这三种情况，warp 在“它实际会走到的指令”上，都是 64 个 thread 一起做，没有任何“只开了部分 active”的时刻。

**Answer:所以最大的 GPU 利用率可以做到 100%。**

---

**问题3：获得最大的 GPU 利用率时，数组 B 的值有什么特征?**

先问：**导致利用率下降**的直接原因是什么
**同一个 warp 里，有的 thread 进 if、有的不进**；或者有的 thread 进内层 if、有的不进。只要出现这种“不一致”，就会有指令在执行时只有部分 thread active，利用率就降。

那要想把利用率顶到 100%，B 需要满足什么
**让每个 warp 内部的 64 个 B[i] 在分支判断上完全一致。**

把“分支判断需要一致”翻译成对 B 的要求，就是下面这几类（按 warp 分组看）：
warp 内 64 个 B 全为 0
外层 if 全不成立，后面块都不执行，不会分歧
warp 内 64 个 B 全为正且非 0
外层 if 全成立；内层 if 因为 C=B^3>0 全不成立；不分歧
warp 内 64 个 B 全为负
外层 if 全成立；内层 if 因为 C=B^3<0 全成立；不分歧

**Answer:一句话总结就是：按 warp 分组后，每组 64 个 B[i] 要么全是 0，要么全正非 0，要么全负；**

---

**问题4：执行整个代码段，最小的 GPU 利用率可能是多少?**

这里先把“怎么算最小”说清楚:

**先问：题目里的“利用率”更像算什么?**
题目给了“busy 的核心占比”，而且强调“每次迭代包含 6 条指令”。这类题通常是在算：执行期间的平均忙碌比例。
也就是把执行过程中每条被发射的指令，看当时 active 了多少 thread，把这些 busy 的比例平均一下。

所以可以用一个清晰的算式表达平均利用率（按一个 warp 看即可，因为我们可以让每个 warp 都长得一样）：
`平均利用率 =（所有被执行指令的 active_threads 之和）÷（被执行指令条数 × 64）`

**再问：哪些指令无论如何都是 64 个 thread 都会执行?**
这段代码里，下面两件事对每个 thread 都会发生：
A[i] = B[i] * B[i] （每个 thread 都要算自己的 A[i]）
if (A[i] > 0) 这个判断本身（比较/分支指令）也必须执行一次才能知道进不进块
**所以最前面的两条指令，active_threads 永远是 64，利用率永远是 100%。这点你无法把它压低。**

**那想把平均利用率压到最低，只能在“外层 if 里面的那些指令”上做文章：让它们执行时 active_threads 尽可能少。**

**外层 if 里面有哪些指令?**
计算 C[i] = A[i] * B[i]
判断 if (C[i] < 0)
可能执行 A[i] = A[i] + 1
执行 A[i] = A[i] - 2

**要让这些指令执行时尽可能少 thread active，最极端的做法是什么?**
**让一个 warp 里只有极少数 thread 满足 A[i] > 0（也就是 B[i] ≠ 0），其他 thread 都让 B[i] = 0，这样它们不会进入外层 if，于是在 if 内的指令执行时它们全是 inactive。**

**能不能做到“只让 1 个 thread 进入外层 if，其余 63 个不进入”?**
可以：在一个 warp 的 64 个 B 里，让 1 个 B 是非 0，另外 63 个 B 都是 0。
这样外层 if 内部的指令执行时 active_threads = 1。

**内层 if 怎么处理能让平均利用率更低?**
如果那唯一一个非 0 的 B 是负数，那么 C=B^3<0，内层 if 成立，会额外多执行一条 A[i]=A[i]+1。
注意：**这条额外指令执行时 active_threads 仍然只有 1，但它会“增加一条低利用率指令的占比”，从平均角度会把平均利用率进一步拉低。**
**所以要追求最小平均利用率，让那 1 个非 0 的 B 取负数更好。**

**于是，一个 warp 的最差配置可以是：**
**63 个 thread：B=0**
**1 个 thread：B<0（并且 |B|<10 仍满足题目）**

现在把这个 warp 的指令执行情况列出来:

必执行的两条（全员 active）
A[i]=B[i]*B[i]：active=64
判断 A[i]>0：active=64

进入外层 if 的那几条（只有 1 个 thread active）
C[i]=A[i]*B[i]：active=1
判断 C[i]<0：active=1
A[i]=A[i]+1：active=1
A[i]=A[i]-2：active=1

这时总共执行了 6 条指令，平均利用率就是：
(64 + 64 + 1 + 1 + 1 + 1) ÷ (6 × 64)
= 132 ÷ 384
= 0.34375
也就是 34.375%

**Answer:所以最小的 GPU 利用率可以到 34.375%。**

---

目前至此完结撒花✿✿ヽ(°▽°)ノ✿

